okay. hmm so we all attended interspeech, no? so how many sessions you attended? i mean did you attend any any session? yeah. that's really that's really your so any general impression or feedback? so f for any of you it's a first conference or maybe for? no, i mean in the sense that it's uh interspeech is really big conference for speech, so did you attend before? uh, ok ok yeah, o ok yeah, so then you're hiding a you have attended before yes. yeah, i see see. yeah, yeah. i t i uh i think it's quite interesting, but only annoying thing is this multiple sessions. and most sometimes you can't able to go to oral talk this oral presentations most of the time, because because like posters is you can spend a lot of time posters looking at m many posters than sitting twenty minutes for one oral presentation. so in t twenty minutes you can see at least two, three posters, and you can directly talk to people, so. even i found like uh very few people in some oral presentations. i think most of the people are like their own posters only. yeah, how presentations are hardly handful of like f five or more. yeah. ah, ok. hmm yeah. ah, okay. yeah. even some panel discussions on that human speech recognition reducing gap between the a_s_r_ and and h_s_r_. it was a bit interesting, lot of arguments and yeah, b differently it's mainly the differences different approaches of engineers versus linguists or phoneticians and yeah mm. ah, okay. yeah, even the panel discussions w i think one is really held in small room, so people were really crowded. yeah, it's yeah,. yeah. ah ok we went to like far south, to lagos and yeah, that wa uh ha yeah. that was really good, those beaches are really good. yeah, even dolphin in the yeah. but the weather was really hot, the south it's more than thirty five. uh. lisbon was good, um little bit bit mm. yeah, even local transport, it's it's. but yeah, most of the time the buses are really crowded. uh. yeah. yeah. yeah, because it's really big city, no? no like. mm. yeah. hmm. yeah. yeah. no, re yeah, but that's that's not good, direct bus is not good like. so you can go to the direct uh ce central place and then occas uh. mm. yeah, da uh what is the place, uh the the central i mean where we change the bus to yeah, in the yeah, maybe you can check the booklet. yeah it's there, like i used to find bus numbers from book only. oh, okay. oh, okay. it's like uh in it's like uh all the semi-tied covariance matrices or yeah. so did they reduce it's in between the direct covariance this is full covariance. should try to reduce some parameters by tying. oh. uh. ah, okay. yeah. mm. in your tie-in. no, i think they got good results with just using diagonals or definitely yeah. yeah. mm. maybe decorrelation again they do d_c_t_ or k_l_t_ or they do that? l_d_a_ or like yeah. so no, but still there is there is still ev that's why like people come yeah, definitely it's not optimum. so it's it's also like doing along along the diagonals all again. it's really computationally really uh yeah so see suppose if thirty nine in te thirty nine to thirty nine for every yeah, yeah, so uh there is a thing yeah. ne no problem, but if you see so many models and so many mixtures yeah. but yeah, thirty you can see you c you can't really like even you can't the models itself is like thirty eight times more than that, so. so y if you have like five megabytes or ten megabytes of models then yeah. but even i think it's really bit like impossible for the really big systems i guess, like storing storing itself. yeah yeah, yeah yeah, it's really ti yeah yeah, for every you're so you're to store all this thirty yeah. so from mm-hmm. mm-hmm. speak uh yeah, different task and so uh i found one paper interesting. i think you know vivek most of you know. uh this is uh vivek's paper on like variable scale feature extraction. normally we use to fix it window for feature extraction, like twenty five milliseconds or thirty seconds. here he's proposing variable scale, because the fixed scale is non-optimal for it's non-optimum because like for vowels you can have m much longer, and for plosives and these things you really shorter, like even around less than twenty milliseconds also. so he's ki proposing like this variable scale window f uh kind of online for each segment. yeah mm mm you can do like you can measure and you can do but he's mm basically doing some mm likelihood ratio testing. so if so the main idea is like suppose you have uh one segment, so he's trying to find the stationality cautious stationality of that segment. yeah, as much as possible like but yeah, definitely you have to assume some l uh like minimum and maximum sizes of your own, so he is using minimum of minimum is uh i think twe i don't twelve point five milliseconds i think. maximum is uh sixty milliseconds. so the minimum no no, i think a minimum is twen twenty milliseconds is i yeah. twelve point five milliseconds is kind of sift. twenty millis minimum is no no, de because of uh no, you can use mol yeah. no, mainly because of uh m_f_c_c_ computation, because yeah, it becomes really noisy, like if you s yeah, ten milliseconds means you have only eighty samples for uh and then you have refused twenty four filters, the filters won't get any samples, so. yeah, because of computation. so even i i think even st uh he can find less than twenty millisecond windows also, especially for plosives uh. shifting yeah, he's using twelve point five millise yeah, shifting is almost same. but the problem like he yeah. yeah, he's keeping same number of frames. uh he's using uh twelve point five yeah, twelve point five millisecond yeah. number of uh frames? w you want to actually the problem's again uh uh you see the shift is the nyquist frequency if you compared modulation spectrum. so again, if you change this shift, the mod the nyquist the modulation spectrum, nyquist frequency changes for each window. so if you want to do again another high-level feature extraction again, it will be problem, so. so but he's uh like he's he discuss i mean describing this, because even this itself is a problem, keeping the f uh fix to frame size, because you're analysing your this uh segment many times. so this shift will suppose if you find one segment of sixty milliseconds, and then you're doing this ten millisecond so every almost i think yeah, mm uh yeah, five frames y you're analysing the previ this already segment so. so again, this may blur some frequency transitions or yeah, but the problem is again this uh bottleneck is here, like you can't change your nyquist frequency modulation and yeah. yeah. yeah. so it's so uh so the main idea is like first to take the some some window, some sam some samples, then he assume somewhere some variable and point there is a change. then then it's not symmetric, i you can cho uh y it's basically uh too like again, do for all the samples in that gyro. so then what he does is like he proposed uh like some like ratio test based on maximum likelihood. so th tha that is really simple, like so what he does is like so he he computes the residual, he first he does the l_p_ analysis and then he computes the residual, and he takes the residual energy of f full signal like gy gy gyro samples. the full signal. so maybe i can the wh the whole window. no, that's that like he assumes yeah, sixty milliseconds or something he start with, so so then he can yeah, yeah. yeah, then he can split that window at point n_ say point n_, so then you will have gyro two no, you can segment it maybe. yeah, so yes, he's supposed this is your p window. so then you can move your like this. you can move your point, so that then this will be like one and this will be second. but again, he assumes some initial uh sizes for these windows. yeah. he don't start with the yeah, he don't s yeah, start with gyro sample or something. he's starting with uh so the l left window is starts at twenty milliseconds, so so you already hear like s suppose this is full signal, you're already here. so right window is should end at twelve point two, so you're to search basically in this range. so yeah. yeah. yeah, and then if you the maximum is sixty millisecond frame for him. so if you don't find anything sixty milliseconds. if you d no, the point is depending on the likelihood ratio. so he got so he supposed this so this is suppose se segment one, and this is segment two. so he compute this error here. so and then he p gives some kind of in terms of this residual letter. so this residual letter is for full gyro samples. then there would be suppose this point is say n_. so gyro to n_ and then like so this the b basically the main he questions or here yeah, he's for finding the likelihood for full frame, and then it's basically likelihood ratio test, so he's comparing the likelihood of the full segment divided by the likelihood of the the sub-segments. likelihood of the uh like this likelihood is uh this error estimate of the residual. so residual power he can compare t so he after do after doing l_p_ he can get a l residual and then he can compare the power of the residual. so that he's proving that power is again maximum likelihood estimate of your l_p_ parameters. so that which interesting it's you don't really need to do a lot of computation, you just need to take the error of n uh residual of this full window and then residual of energy of the sub-windows and then you can just divide them and then the only thing is like again he has to use some threshold to decide, that that's only the p problem. he got uh he's founding something more uh around three or three point five e_s_t_ optimum threshold, but the and another advantage is like it's not really changing speech recognition, whatever, so not really changing because of the threshold, so. uh you don't really need to f like fiddle with threshold a lot, so mm. yeah. yeah. yeah. yeah. yeah. yeah, this is yeah, i understand. this is this is the actually basically some theoretical proof, like maybe if you want threshold you can see and he's coding from uh leven like uh. no, this is from again yeah, statistical signal processing. what they say here is like suppose if you analyse two distinct or uh this l_p_ analysis in the same stationary analysis window, the coding will always be greater than the ones resulting from this analysis in two windows. two stationary windows. so the he's basically based on this theorem, so so what he's saying is like if you do this error, it will be always greater than the mm th these errors of two stationary windows. yeah. yeah. yeah yeah yeah yeah. yeah, this maybe i think yeah, this he's getting some improvement, but yeah yeah yeah yeah. so he's just uh uh he's normalising the energy coefficient, because energy is really affected a lot. uh so he's li energy like, because c_ gyro component like, so he's using m_f_c_c_ so he's normalising the c_ gyro component for this t yeah. power is again like yeah yeah yeah yeah, it's time like yeah. it's okay like samples, or you can see root mean square energy or something. yeah. yeah. no. no, n no, you vo what you mean like the window lent. the sixty mil no, it i it doesn't really uh but i think it's he he doesn't use i guess, because uh uh then like you just you take these features and you train model, so. it really matters wh what frame shift you're using for models, like because how many he needs to use same freq no no, wait. no, he will get like every ten milliseconds he get one fra one features, like he it doesn't really depend, because suppose if you take your t case, like you your window i maybe longer, so doesn't really matter like how what size window you use or not. but i tell uh at every ten milliseconds whether you give some features or not really matters, no? like so. so it doe you can use uh thirty milliseconds or fifty millisecond window. same. yeah yeah yeah. yeah yeah yeah yeah yeah. yeah yeah, framing it's like it's really yeah. yeah, if you have if you're change framing, it's really i think even it's really problem, no? like if you try even models also i guess, if you change. it's not only with this model, it's in spectra and shifts in deltas, because yeah, but it's really complicated i think. if you use yeah, that becomes really yeah. but this is kind of interesting. yeah yeah yeah, the temporal dec. mm-hmm. yeah. yeah. yeah. yeah. yeah. yeah, first this is proposed by like uh even we read one paper in our reading group, you remember that? i have implemented, it's quite working well, like uh yeah. mm. yeah. yeah, honza is also worked for his be yeah. uh i yeah. ne he was re he was quoting but that yeah, he was uh quoting that paper also, like v he was telling this uh this is kind of optimisation criteria what this temporal decomposition is doing like. you're trying to segment your s signal into like discreet windows and then so but he was telling like this uh relationship between optimation and and then cautious stationality is not really obvious. so stationality is again different, no? stationality is this is optimation, we want to like see some signal, few segments, which can really represent whole, so maybe that's why like this may not be really yeah. yeah, this one yeah, he's getting some improvement n uh this is a, yeah. no no no, they again but again uh, getting that ten papers is really difficult task. it takes maybe years and again uh it definitely like who will who will choose the ten papers like, and if we asked yeah, but yeah, those things are practically kind of really but i think this kind of things what you showed i mean what you said is really good, like if people start implementing like some people propose something in feature level, some propose in something in model level, but these two are really independent, no? when really work combine these two or yeah, especially features, like suppose if somebody come with different good features, again people st again use m_f_c_c_s no no, it it's not really sure also, make suppose uh people you can't really force people to use same features, so so they wo they will be happy with their own features and their own scripts, so their bit rate like tend to change features every time and it's hmm. but even nist you can yeah. yeah, yeah. then so at lea especially l_v_c_s_r_ in really big systems, then people yeah. yeah. because yeah, it's it's again you don't know. oh. no, then uh there will be only one conference in three years or something for speech, because but again like that's so you can publish only once in three years or yeah. but even people are doing suppose in interspeech there was uh some challenge for speech synthesis, so so what's is uh like it challenge was really good, like sup they give the database, so you have to work on that database. what technique t use is it's your choice like, but that database and then the results analysis is they decide, so. even speech recognition also some tests are coming, like for phone segmentation or something. people give some database and then you have to dis you can use whatever you want and you have to produce even for features also like features also i think yeah yeah, the they they designed the data such a way that it's it's really like really real uh data. no no, for s nay, speech synthesis it's like they give you some data, so whether like you use for training or l you can you can do like data driven or like model based approach or whatever like. at the end then they will ask you to synthesise some sentence and you have to synthesise them and then you have to send them your training data, onl no development, training, no yeah, training evaluate it's again subject to s uh because for speech synthes yeah. yeah, listen and yeah d yeah, even uh they it's mostly they use native speakers only, because they can really judge well, so. yeah yeah yeah yeah. yeah, yeah yeah. so even they propose some kind of word error rate. so you have the original speech, so i the synthesised speech, is there any words which are not matching? so even speech synthesis, they're also uh introduced this w_e_r_ term, so. uh. but this was really quite successful like in this conference, interspeech. so a lot of people participated in uh i think even they're continuing this for uh next year and text to speech. but uh i heard like even for i_cassp next year there is some com uh like computation by martin krug, sheffield, and on this feature extraction stuff and so at least if you make task simple focus, then it may be good to compare these features and but if you s use some uh large vocabulary system, it takes t six months to build and at the end you don't know whether your features are really uh so. so maybe that's for like people are always using p_l_p_s or m_f_c_c_s, it's it's such a b like lot of time involved, so you can't really check many thi yeah, yeah yeah yeah. but at least in idiap we have this numbers recogniser is almost kind of free, so everybody is using so that's what we are doing, no? almost almost like we are putting different features and i think yeah, yeah, numbers recogniser is not really different from like uh a f at least in idiap we have almost same, but maybe you are using twenty nine, that's for you it's different, but one more but at least i twenty seven. hemant and we're all using twenty seven, so. yeah. yeah. no, but the problem is you're using only digits, so maybe that's how you no, but test set is digits, your m main task is digits k yeah, that's uh like but o_g_i_ numbers are there like now we're little bit convulsed, like we're using twenty seven phones and before it was like twenty four, twenty five. but it's better, like once you have like whatever the back end, then you can put features like whether they're gammas or like spectral entropies or m_f_c_c_s or whatever. then at least you can see uh? yeah yeah yeah. but aurora is aurora task. so but aurora, is it really big database, or i mean how much time it takes to set up system and then it's fast, no? but the problem is aurora again, these models are word based models, no? yeah, so again here we use triphones and so but definitely, i d if we want to show noise setting, it's you have to show on aurora also, like aurora is real time. yeah. yeah. yeah, yeah yeah. but yeah yeah, at least i mean wha those studies i think people already did for even in i_c_ i_c_s_l_p_ two thousand two there is special session on features for aurora, so yeah, yeah, i yeah yeah. this icsi features and this uh s uh s uh o_g_i_ features and all this. so there are already people but again like uh, at the end like for l_v_c_s_r_ people are using p_l_p_s or m_f_c_c_s or nothing of these fancy feature yeah. new but maybe my have a f for publishing another paper or something. mm. yeah. but maybe guillaume, you can tell, no? like uh you worked i with these siemens people or no, siemens or these daimler chrystler. what kind of features do you use, like do you use um any you don't yeah, you don't ah ok okay. okay. ah ok yeah, even s yeah. i didn't discuss anything with sunil also, like i dunno, technical discussions, anything. these kind of things are a bit, yeah. yeah, yeah. but do you have any like feeling that whether they go for all these new features or they usually use only mm. maybe yeah. mm-hmm. hmm. but even they don't believe like with one paper or two i guess, because at least for them no p_l_p_s or m_f_c_c_s, they know that okay, these things work on every task, so okay, we can pu put hands on these things yeah. huh? yeah, yeah yeah yeah. maybe like yeah, f because sunil is working in nokia, he's a expert in kind of features or yeah, it l i think. but yeah, we're not sure, even sunil won't tell us. no, no n no, this uh cohen, you mean like uh uh hmm. oh ok huh. uh. but this was signal company, no like? this voice signal company, hy hynek always mentions uh cohen's company with what they're using, you n yeah. voice no no, voice signal. they make uh recognition for these samsung phones and yeah. uh they have like really s many recognisers i guess. maybe definitely they may be using p_l_p_s or you know. yeah rasta or rasta or something you mean. mm. yeah, because mm. yeah, but again yeah, too new is really definitely because like want to at least see i at least four, five years of results of some new features and some consistent results or ah. uh space or ah, okay. yeah yeah yeah. mm. yeah. but yeah, d yeah, uh another issue is like again the computational issues, no? if they want to make on mobile, they don't want really use some fancy, really expensive computationally expensive features or all these yeah yeah, it it's just phone recognition or something like yeah. but most of the time mobiles, they use d d_t_w_ kind of thing, no like? all these uh so voice calling yeah, voice calling and these thing, because that's easy like, because they don't really need to put lot of uh memory and these things, otherwise if they want to build a real yeah yeah, yeah. where where? mm. yeah yeah. yeah, cohen, yeah, yeah. it's samsung, some some new really new phone like, mm. yeah. but uh the software is always available or like no, the software is again on top of mobile, or like it comes with your mobile no, the thing is usually these kind of these kind of facilities are like no not normal, so so they may charge more or like yeah, yeah. yeah. ah, ok yeah, definitely they may charge for more money f again, some extra bill for this using this recognition engine or something. yeah. yeah. yeah. yeah, but maybe like if it can't really recognise you it will be really annoying it's better. yeah, i have used once like this dragon, that was kind of good like, yeah. because even in when i was working in uh like in edinburgh like, one guy, he use i he got some problem with hands and then he was always using this n uh dictation even he used to write lot of c_ plus plus code with the dictation like. it's really good like, so. but you ought to really train and then you're to kind of uh yeah. i get used to the system. yeah yeah, it's really like you ought to get used to the system, like you how to say all the callings and this thing system how t but it's really funny like, he is able to use it for like like years, i mean he used to write a lot of code and, you know, it's really great. yeah yeah, you can listen festival. read the like mm yeah. yeah yeah, fes yeah, i think i think you can do the festival, you can just call like uh this there are different kind of again, how you call the festival, so you can just give the full there is an too. so then you will get the all the caller and other things. if if you want to like read full text, then you can say like the full text, like those syntax and then it will speak till it finishes the text. yeah, it's real it's yeah. diphone, yeah, yeah. that's why it's real time, like otherwise it's bit difficult and it's okay, you can easily understand, so it's intelligible, so. uh festival is good, like now it comes at all the linux boxes also, so in so already in this no the there are so many voices, like again which voice you want to use. no, most of the the festival, uh what you get on linux machines, is diphone based only. so they supply few voices. yeah yeah, yeah, just uh concatenation of the no, it's already there, if you just put li festival, it comes wi because it's comes with direct linux software and you know, it is open source. festival is open it's yeah, cat voice, like there are like they they have few yeah, new voices like, yeah. yeah, that's the difference that tha in views this unit selection. it's not diphones, you have really large inventory of the speech. uh those voices, they're not yet released. yeah. no no, it's free, they'll be releasing soon i guess. so maybe you can just check no, i it's it's here on the web, then it won't come with your linux software, so you have to again download from uh their web page. yeah, you can download, now. yeah, yeah yeah. it the voices are called multisyn, multisyn uh sl uh that's some voice. so you can download from the voice from the web festival. yeah yeah yeah, yeah. yeah yeah, yeah. so voice is like the database of, the parallel speaker who speaks that this text, so. it was really large database, so it the voi the quality will be better because you can find similar yeah, it's more or less but not exactly like uh it's two times or one point five times or something. but you can prune like how much you want, because so this is again like uh that's kind of my p_h_d_ work how in this system like how you choose the units and how you concatenate. so there are again cost functions and viterbi. so you can you can always optimise, you can put some thresholds and pruning and then you can make it real time. so then again, compromise uh between the quality and yeah. but still it's good like now. th you can just check multisyn and then voices. uh yeah, yeah, it's really good. yeah yeah yeah yeah. but only sometimes like it can't really call this yeah. uh standard text. yeah. yeah, ascii file is ascii and but only the problem's again acronyms and sometimes it expands, sometimes it may not expand, because it's not in their dictionary, so yeah. it's not in the dictionary, like if we again it's problem like, so. but those things you can add, like if you're really familiar with festival, so you can add always these yeah. but at least these things okay like, it won't really make mistakes so often like, sometimes alan black and paul taylor, they started r festival in ninety six or something. edinburgh c_s_t_r_, yeah. uh f oh uh this ma this johann waters or like no, they d what they did portland o_g_i_, they did l_p_c_ based synthesis. the festival is alrea already like it's one first you start with th ah, okay. mm. from. i don't know. no no, actually the b ah, okay, okay. yeah, maybe, because ah, ok hmm. yeah, first they started in c_s_t_r_ with alan black and paul taylor, then then it expand to many places, because if somebody is fil no, simon king is he's one of the others, but rob clark is the man that you met um. uh uh, already. yeah. it's so anything uh more about lisbon or interspeech? i it's enough, yeah. okay.
okay. so you start i think, right? yeah, so we are going to talk about the papers from interspeech. this is not a good question, i think. all of them, of course. all of them, of course. oh yeah, that's true. okay. bad answer, right? like yeah, i it's not every year, right. yeah, we met in also in i_c_s_l_p_, right? the first time, so. yeah. so how how did you like the conference anyway? when in compared to to th the others, the previous one. but yeah. right. oh yeah. he was from australia, right? the guy yeah. uh he's very famous i think for that, right? he's the the inventor of that implant and for for the ear i think. s yeah. mm. so you went to a beach? to a like atlantic sea coast or yeah? uh-huh. uh it's not so far from lisbon, half an hour, mm-hmm. uh northwest mm. yeah. yeah. but i think lisbon those days are pretty good, like seem to me quite not cold but still okay, like reasonable. oh yeah. yeah, i was happy to be back in suisse after few few days it's better to be here i think. yeah, it's big city, many people and yeah, yeah. those new only, right? not the old one, but yeah. yeah. ah it's not necessary to have it, maybe. some time, yes, but not so many days or oh yeah, yeah, that's true. conference. yeah, yeah. it's better to ta yeah. the easiest way. i don't remember the name. is it there? yeah, i it's supposed to be there. oh, you've got the s yeah. uh-huh. like no, i don right, mm-hmm. yeah. so they obtain better results with that finally or but right. but if you compare no, if you compare it with some baseline, let's say you use just g_m_m_ with diagonal covariance matrices a and no decorrelation before like uh yeah. or l_d_a_ is there or yeah. but but there must be the sense, right? because once you decorrelate the data, then you don't need to have full i know, that's so that's that means that the decorrelation is not uh optimal, right? so so that's the reason. mm-hmm. yeah, yeah. it is, yeah. uh no, there there is not yeah. right. right, right. yeah, yeah. yeah, yeah, definitely. it's almost impossible for like l_v_c_ aside, impossible to use it, right? you've got dimension thirty nine times thirty nine? yeah? so yeah. it's impossible to use it. and you cannot even train it, right? more or less i think. oh, so that's interesting. mm-hmm. much longer. shorter, yeah. right. wh what is the minimum and maximum? so it's not so mm two point five milliseconds. twenty milliseconds? from oh, yeah. right. sure. and the shifting is still the same, or not? o yeah, yeah, overlap. so like um no, how many frames per per second you do have? is it again like uh one hundred frames, or it's less or even if you keep variable length of the frames, right? you can still keep the same frame rate. because i understand this would have sense for, i don't know, speech coding where you want to preserve uh or you want to encode it into less frames, right, but why do why to do it for speech recognition? it means that uh yeah, no, wha why to keep variabl variability in the length of the frames? wh why not to keep the same length, what is the yeah. right. right, right, right. mm-hmm. right, right, right, ri right. mm-hmm. mm-hmm. mm-hmm. right, yes. right, yeah. mm-hmm. right. mm-hmm. mm-hmm. mm. mm-hmm. well mm-hmm. yeah. yeah, probably. no, i don't think so. there is no information. hmm. yeah yeah yeah, right, right, right. yeah, yeah. yeah, it might be difficult. mm mm-hmm, probably. who knows, maybe it's possible to use it somehow, but then there is uh information about temporal information is somehow included in such yeah. yeah. there is also some um like uh not paper, but i saw the algorithm i have been using even that called temporal decomposition, i don't know if you kn you know that like uh i don't know who propose it, i just had it from from beambot for like a beambot, do you know the guy? he's french um somewhere now in teleconference like mm i don't know where he's working, but th we were using it for uh speech coding, so we just had a speech and you had decompose a speech into such segments, temporal segments which were stationary inside. and then you can more or less uh quantise those segments somehow and use it for encoding let's say, or and it worked prett yeah, yeah. they were using like s_v_d_ stuff, singular value decomposition for that and it worked pretty well, like i was surprised like. yeah, yeah, exactly, uh he was using that. yeah. oh well, it's it has been used for speech coding. nob nobody use it for recognition stuff i think, i never heard that. no, i don't of course. yeah, that's good question. maybe. right. right. mm-hmm. yeah, that's true. mm-hmm. mm-hmm. mm-hmm. that's true. that's important. all the papers. okay. i think people wouldn't like it. many people wouldn't like it. right. yeah, yeah, yeah sure. exactly. right. and everybody's uh like uh using different training and testing data and then you don't know if like they show it works for timit, but then you try to use some different databases, you see it doesn't work. yeah, sure. yeah, yeah, you can. yeah, exactly, yeah yeah yeah. nist evaluations. yeah, but nobody you know, it's those new systems are still not general, like you know, like those p_l_p_ and m_f_c_c_s, because everybody knows that di it worked somehow, right, for any kind of data. that's why they're comparing to to that, so. once somebody will come with something new, okay, he he's showing it works for some datas, but still yeah, yeah. first it's difficult to to show that it works for all all the datas, right, because it's really and yeah. yeah, that's your choice. mm-hmm. no no, i just for yeah. i think so, yeah, probably, mm. mm-hmm. mm-hmm. ri i it's just t_t_s_, right, text to speech? right. mm-hmm. right, right. are they? for o_g_i_s? for o_g_i_ numbers? different setups? like some people use this setup and some people that? uh-huh. so you just oh yeah. no, uh it's. i dunno, i i've been working with that and right, yeah, yeah. for noise conditions, yeah, it's pretty good. well, th those f recognisers are are made there or just you just use them, that's it. you don't have to play with it. she even is f it was forbidden in the time, right, so you have recogniser, just use it. don't play with it you just play with the features. yeah, right, exactly. so. yeah, yeah. ri right. yeah, yeah, i remember. with david pearce he was. yeah yeah, exactly. we are using those results more or less. but not everybody go. not everybody. many people stay, so. yeah. yeah, sure. but exactly. no. well, many people place with tandem, right, like m_i_t_ or who is using that some not m_i_t_, but somebody i dunno, hynek told told us that somebody's using that um very actively, like okay, it it works for them and and they are close to like industry like uh you know, there is it's a research group of i know, i don't think it's icsi. somebody else or i don't know, somebody new uh s but i think some t somebody like a_t_ and a_t_ and t_ or what? oh qu yeah. you me i don't know, he's even if qualcomm? no. now y oh yeah yeah. for right, right. no, i don't know if it's yeah, i think so. yeah, definitely. maybe they are using p_l_p_s and but i th i believe that there is still some stuff up to yeah, for rasta or whatever else, like all those decorrelations and transformations. i think it's there, but more or less it's again based on m_f_c_c_s or something like that. or maybe traps. no, more or less i think yeah. right. that's true, it's true. but that's different situation, because right? they need that it doesn't create that much, yeah. if something happened with the machine and uh i don't kn the alloc system, it doesn't matter, right? somebody will fix it and change it, but sure. yeah. even even not viterbi, just even something simpler. yeah. uh i don't know. do you think so? yeah? oh yeah yeah, yeah yeah, that's true. yeah, sure. yeah. even on the phone. n some new one or from that c cohen's? oh, uh-huh. and but which uh phone it is? f samsung, something? where uh some but some which you can buy, right? it's it and you can train it, like only your speech or it's general uh it's quite interesting, yeah. okay, uh-huh. or you you can just buy that application, right? i don't know, like it's like you you've got some mobile phone, and then you can buy from another company some such application wh you can dictate your s_m_s_ or whatever. s i don't know if it is done now, but like but have you ever used like dictation system for windows? those uh there were many of them. yeah, drag the mouse there. and i heard that it worked very very well. um alright. right, newer speech, okay. okay. how to say okay. mm-hmm. i think, yeah yeah, definitely. it's based on the diphones diphones, right? yeah, yeah. yeah, i think it's very good, yeah. mm-hmm. so you've got it installed in your machine here or festival, right? okay. oh. a and have you have you tried it? and? so you can uh-huh. mm-hmm. yeah, it's very difficult and yeah, yeah. and you just uh have chosen some text? any text you just put in and okay. and it can read uh how does it read, from which um from which uh how to state it? uh is it uh possible to read p_d_f_s or just standard text? whatever, t_x_t_, right, files. uh-huh. yeah. well let's oh really? anyway, who is the author of festival? edinburgh? and i thought it that's the guy from c_s_l_u_, from portland, who was there during f yeah, but there was no, no, but that that was the guy who's on a wheelchair now, he cannot m move even, he's very in like indic how to say that? like he cannot have he just can speak only through some festival system or something like that. he was the auth no, yeah, h he was working i think in w o_g_i_ even, he's very famous, but i thought that he was the author no, it was developed for him, like he was just first tester i think of the system. something like that. like it's not t text to speech, it's just the uh uh synthesis, which means which is more or less the same, but i dunno. yeah. so it's somebody and simon king is playing with that, right, the t guy. okay, okay. i think that's enough. no other papers it's just one paper and and festival. from linux, it's very im interesting. yeah.
and it's always the other way around, that's how you put it on. i think the the most of the bad words are at the beginning when the people try to put in on. huh. whoo. ah, i cannot put it. oh no no, not at all. well geneva, that's why i'm here at idiap, because i attended geneva eurospeech. i c why, why you cannot? mm-hmm. exactly. meet okay, so you attended mostly posters, i guess. so how many presentation you you i realised it were that there were panel discussions only at the place where i could see the the t t you know, there were some boards saying that there is some panel discussion, uh but i i couldn't get no one knew where is it, what it is, where it happens and why and so forth. it was com yeah. finally i found, but well, this is not this speech is not restricted only to papers and these things, you know. so how did you like lisbon then? yeah. yeah, w we went to the costa caparica there, with the bus. we we were we were supposed to take take the direct one, but we didn't. north? okay, so you stayed at the at the same coast, not not you didn't go to the island, cross the river where where is the bridge and and then okay, so that was a different place. yeah, with hamed uh hemant and these people. the more south to th the hole through the water. huh. i was i was very surprised that even the trams did have a a_c_ there. i couldn't see it at at all. yeah, r right, but can you imagine something like in? uh it's a_c_. well, it it was much better if if if you would have to spend a hour and a half that uh hemant and the others spent while going with a ca uh with a bus to to the conference centre from from the the hotel that we were. there was a direct bus and it took them hour and a half then. mm. yeah. yeah, that was the right choice, but to take something which which was uh direct, in quotes. it was n not the f direct as anything else, but. uh it sounds sounds like. let's go on, this was just a of what? pruning of covariance matrix. but what what does it mean tied then? uh yeah, but then you have to store the whole matrix and plus some extra information what is tied to what or b yeah, but uh i don't know about anything like this, but tying. mm. but ooh the the main point is to to uh like make it faster, the the the decoding, or what? well that's possible, why why it should be i impossible? so it's completely impossible to do it with full covariance matrix? why? yeah, computationally e expensive, but of course, from the point of view of computation, but otherwise it's there's no problem, no? with okay. okay, so so here you have to choose either many many things to store and huge computation uh yeah, yeah. no. uh depends. so. so there are no results there? no hmm yeah. uh-huh. yeah. yeah. mm-hmm. okay. why it has to be online? okay. like extended as as more as much as possible to keep the okay. of course, of course. whoo. yeah, it's right. this is maybe because of f_f_t_u_ or these things. it's no not as small as as it should have been, but mayb maybe because of other processing after. no? yeah. well noisy, yes. overlap you mean. uh w how do you mean to say? uh um i have two questions, first is essentially how he's doing in that. no, well uh once he gets the frame, like from here to here, then where he advances to to start with c computation of the next frame? yeah, mm-hmm. okay. okay. okay. mm-hmm. fu ok okay, so do what do you mean full? some some sub-window or the the whole ten seconds of speech or what? it has to be also some somehow okay, something longer window than the one we are speaking about, okay? uh w okay, so you have a window and then you split it to s two parts and you shift the the point where you split it or okay. okay, splitting point, okay. okay. okay, so basically we have two fixed points. yeah, okay. okay. okay, so once you have say one hour of speech, you start at the beginning, and first you try with twenty milliseconds. then you shift to twenty five and so forth until you okay, and and you find the point where where what? where where do you stop actually? yeah, mm i know, but you start at twenty and go to sixty, but where which point do you choose? uh okay, but which which likelihood ratio? okay. yeah, but uh likelihood, that's something and something. w what is likelihood? okay. yeah, but it has to yeah. okay. so it's basic so mm. yeah. yeah, but still. if mm yeah, but why do the main value which are comparing to, the the the overall value or overall likelihood or whatever you call it, why he's using the whole signal for this? because so if the whole signal was uh steady somehow and it it would be able to be modelled by l_p_ model, well, then the residue will be very low. so he's supposing that the whole signal won't be able to to be captured by l_p_ model and that the residue would be high enough or ho wh why he's comparing the o to to the overall? you know, i i don't see the point of dividing over the the whole stuff. y well, sounds sounds reasonable. i see, i see, i see. okay, that's reasonable. because actually if it if if uh the whole signal would be able to be modelled by l_ uh by l_p_c_ then anyway he has to design some some framing. so even if it's very stationary by d um by dividing over this whole stuff, he is able to find some reasonable boundaries. um m ma makes sense um. okay. and mm. that has to be power, not energy definitely, no? yeah, but it has to be normalised. by the length. then power, instead of energy. energy over length is power. yeah, yeah. duh duh duh duh duh. okay. so but this so two things i still don't understand, thi this is supposed to be only for m_f_c_c_s and these easy features ah, one question i i wanted to ask before, does does he preserve within the feature vector the framing he's using finally, because does he somehow put the information about was the size about the frame to the feature vector itself or he's throwing away this framing? something like this, because the recogniser could be good for the recogniser to know what was the chosen framing by this extraction. okay. so so it n uh it might be bad for then for the back end, it might screw up the the t speech rate or normaliser. huh. oh, probably i didn't get it too, so the framing is actually equi-distant. just the windows are wider oh ah, i see. because it would be crazy, yeah, a little bit. oh i see, so so it's it's like this. yeah. i cannot imagine the transformation to get the modulation spectrum out of such a stuff. uh. mm. so why don't you still use it? if it works so well. yeah, but you do the speech coding, so do you still use it? why? mm. uh. uh it's interesting, this. well there should be after each conference there should be something like ten people should sit down, read the papers, and then rank them. you know, a all the all the all the similar papers within one session say sp uh focus to this topic, and say this is the best, this works slightly worse than this one, and throw away what is not that good and and th the first three three papers which are the best shou should be implemented and used from from that time onwards, like forget the all the m_f_c_c_s, because it's too old, and then start building the story on the new stuff and not 'cause it's yeah, it is but i know, i know, uh th this is just a theory. because mm yeah. yes, and i want to yeah. yeah. it looks like mm. yeah? yeah. because to me it seems that everyone is still comparing to p_l_p_s or m_f_c_c_s, and there are so many new systems and everything is new, but it baseline is still uh t twenty years old and yeah? uh yeah. no okay, so so it should be every papers then should contain all the results from nist evaluation or some standard task then. otherwise uh uh that's the other yeah. okay, so it could be you you would have to publish the results on this standard task, and then you could say well, i also tried on this and this yeah. you know, but then then only it makes some progress, this. mm. yeah, but can for for example can you use from some data from from the l_t_ world? for training say? yeah, but they give you also training data or only the testing data? oh, i uh since this is only okay. okay, so so they give you only the training or development data only? mm-hmm. yeah. hmm. mm. hmm. mm-hmm. mm. uh. al almost. yeah. we are a couple of people at idiap and there are already two s two different setups. stories and numbers, or yeah. uh i think we use di distinct train and test set, even se the different sets of phonemes. different well, i don't use only digits. my m_l_p_s are trained on everything and uh test set is digits, yeah. for for what? ah. yeah. yeah, you cannot play with that. because of the voices wouldn't be comparable then. uh. hmm. because who is the one who makes who actually uses what we d try to do here? these are companies or because it's not scientists, scientists just start from p_l_p_ and then design the new features, new back end, or new anything, but they start from the old stuff. but who is the one who is using these results which we publish and try to make yeah, but once you once you finish your p_h_d_ you go and it's over. and those who don't? they stay and publish another papers and they but they they don't turn into a life, no? into life. mm uh-huh. because i i won yes. i wonder which level they are using out of what is there. hmm. yeah. they grab hmm. hmm. okay, so we can hope that nokia will be using tandem tandem p_l_p_s. yeah, i cannot. huh. so it's not related to icsi. yeah, but it was mm. hmm. hmm. hmm. because they grab whatever is certain to work, and they they use it. they don't use anything which is to uh too new which is not that safe. it's like a it's like a nasa, you know, they use four, eight four, eight, six, machines to to to to put two n uh space ships, because it's it's known to work perfectly well or pentium, not any p_ four or whatever, because it's safe, it has been working for twenty years, and then only it safe to put well not twenty but it's always yeah yeah, of course. of course. mm yeah. mm. hynek also told about this cohen ce cell phone, which he was showing to to work l uh with the l_v_c_s_r_. he he was saying that there is something really really uh simple, the decoder i mean, not the features, but he was mentioning something to the decoder. this is just there is almo yeah, something like no d_t_w_ or something, i don't know, i'm not really sure, but he mention that hmm? that was impressive. yeah, but it's that's a similar one to what herve has, for example some he was showing it uh live on m_l_m_i_ c last m_l_m_i_ conference. you can look at the video and play it uh. it uh i'm not sure. yes, commercial phone. what do you mean, always? well, they they just bought a mobile, and based on the o_s_ it's using they they they developed the software. no, this is the development phone, this is not the commercial stuff with the software. they just use the device. by by what? i don't know if it's already the case right now. you had to get used to the system, not the system to you. it's nice. by the way, you should be the one to ask is there anything if i'm just a p private person and i want for free any software which is able to read me a book in english, uh for example that festival, am i able to put the text to the system and to in real time to listen to the output? festival is able to to to read just a mm normal plain text in reasonable comprehensible english or so that i can understand it? non-native speaker? mm okay. yeah. and it's in real time on a reasonable machine? okay. okay. 'cause the the diphone that i was tried to install it and there there are the voices like cat voice and these voices, this is not the one? because uh i don't know which which um what i what is running where uh if it's diphone or which uh approach this is. yes? so it's uh just uh wha samples of the diphones, and it choo chooses yeah, on my on my laptop. or here. yeah, you can you can easily yes, i d i've tried, but there are some built-in like easy voices which you can y uh there is a default one completely synthetic, which you cannot understand at all, and th then there is a diphone synthesis i think it is the diphone synthesis from the some like something like cat w voice and somethi this is so-so, but it's this doesn't sound very nice, and we were playing at the uh summer school with them, actually with the authors, and they were playing us some other other approaches, and it it sounded like a human, it was great, but yeah. so this is not this is not for free or something. but it it's not here so far. okay, i'm i'm fine with this, but i can make it run on my machine. okay. okay. and and the engine is is in the festival itself. so it's just a voice to to okay. okay. mm-hmm. mm mm. okay. hmm. mm-hmm. hmm. okay. uh it's right, i mean yeah, i just s use the douglas adams, uh uh that that book, you know, the mm how it's called? just general english textbook, i i cho i tried. no, the t_ t_x_t_. yeah, ascii file basically. you have to but you can take a p_d_f_ and convert p_d_f_ to ascii. yeah. it was a line with hashes and it was hash hash hash hash hash hash hash hash hash hash. uh i'm not, then i don't want to go too much into details, just a little bit. mm well i can prepare s sorry. these are the edinburgh people, no? mm-hmm, yeah, voice. that i know. um uh. n uh. n uh. mm.
at this precise time, yeah. uh you can't attend multiple session at the same time, so. hmm. mm-hmm, yeah in. hmm. yeah. hmm. yeah. hmm. but yeah. yeah. i liked the invited speaker um about the implant in the ear. it's quite good, no? i think so, yeah. yeah. mm. hmm. hmm. no quite good uh. a bit hot still, but the bea the beach nearby is quite nice, yeah. you should try it, i don't know. did you go around? before or after, yeah. okay. yeah, 'cause i yeah, i stayed i stayed two days before, so uh i went you ju you just take the train, maybe that's the same, i'm not sure. train north. half an hour train, yeah. yeah. west or north, i don't know uh. no no no, not very far. no no. it's different, yeah. yeah. yeah. hmm. hmm. well y we can take the tram and a train. it's it's all conditioned. yeah. ah. mm.. ah, sorry. no no no it's uh or s i'm not sure. yeah. yeah, originally i brought it because i marked some papers, but yeah. onl only o one which surprised me a bit, it was uh speaker recognition i think, but it doesn't really matter. they were trying to model uh um covariance of different components for g_m_m_, instead of using a diagonal. but then if you use a full it's too much. so they use some approach where they tie them automatically and uh after that well, it's more like tying it is still full, but different um components are tied, different components of the matrix are tied. equal. uh no no, you have to store a minimum number a reduced number of parameters, and they are tied in a linear way actually, sorry, not equal, but it doesn't yeah, exactly, that's it uh. so they ju yeah. yeah. interesting thing is that they applied it on speaker recognition, and on the features before they had three different ways to um decorrelate the features, and and this they showed it uh without this, with diagonal g_m_m_ or with this, and with this, with uh semi-tied covariance matrices, it was a less much less sensitive to uh which um decorrelation procedure you choose. if y so. a a also better, but of course it depends on the number of of parameters you put yeah. yeah. yeah, i i think i think it was even slightly better, yeah. maybe i can find it. l_d_a_, yeah, l_d_a_. make it possible to use uh a full covariance matrix. yeah. yeah it's this one. it's i improved covariance modelling. yeah. no, but in the proceedings you can find them. they are better, yeah. they tried p_c_a_, l_d_a_, m_l_l_t_. i i it's not really anything new, but uh the mm they just applied that to speaker i_d_. yeah. yeah. i isn't it smaller, the minimum, no? i don't know, i just mm. mm. yeah, okay. but it's not uh an intrinsic limitation, it's just because of ri yeah. okay. so it's not n it's not symmetric necessarily? 'kay. so he's maximising that or mm mm. mm-hmm. hmm. yeah. mm. okay. mm-hmm. yeah, ho yeah. how did you cope with what you mentioned some time before, that if it's a very long session for each twelve point five frame you will actually get the same segment now. so how d ah, okay. mm-hmm. mm. sorry. mm uh what it's a little bit what nist is trying to do, no? so. yeah, bu yeah, but then we all optimise uh on the same task. so you cannot get anything new out of that after some time. yeah, but then you run out of time. no, i ideally you're correct, uh but mm. so yeah. ho how how do they how do they evaluate? i just don't know. so they ask peop they ask people to sit and listen basically? yeah? hmm. okay. but they d they don't have any measure like you have an original speech, you transcribe it and okay. hmm. hmm. hmm. mm. so maybe well, but then well no, if somebody like nist will have one recogniser and you just plug a different feature, but uh it's dangerous. right. mm. hmm. yeah. yeah. hem hemant is using difference seems hmm. mm. yeah. would be mm would be nice for aurora maybe to have that. aur aurora. yeah. no no, but yeah, yeah. mm. hmm. yeah. hmm. yeah, daimler, yeah. i don't know. you know, they're extremely secretive. i had a hard time just to get the signals. so i asked other questions but uh it's quite tough, yeah. yeah. yeah, they have to yeah. they have yeah, they have to do some test in their mercedes, some speech recognition test, mm but it seems now they mm at least daimler, they mostly work on dialogue management, uh but not much on features and recognition. no. yeah. i d i mm. i doubt they are trying right now. i doubt that at daimler they are trying right now, but maybe other companies, mm. uh. cohen? jorda jordan cohen. no no. mm. mm. but there you could send an s_m_s_ i think, or something like that. uh you could dictate text, so. on a phone, yeah. the phone that uh cohen yeah, voice signals. yeah. i don't know, that's okay. hmm. so it's still still real time? hmm. ah okay, but but not much, yeah. yeah, yeah. alright. yeah, yeah. yeah. okay.
