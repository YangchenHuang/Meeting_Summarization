yeah, still doesn't feel natural. i i wish, you know, the the the the room would just detect when an when a meeting's started it starting. voila cool. so you better start. it was your idea. so. well maybe uh is is the white-board working? or or not. um y maybe show us the general sort of outline. you keep w okay, you can just write it on the paper. yeah. so is this different from the original p_l_p_ two stuff then, is that? uh 'cause that's what you started with, wasn't it? this this stuff that marius okay. okay. yeah, yeah. h he's parameterizing it both temporally and across the spectrum, yep. yeah. yeah. yep, okay. yep. yeah, to that. yep, yep, yep. mm yeah, but it's a it's a it's a different linear prediction model, isn't it. it's a complex linear prediction or whatever. yeah. yeah. yeah. yeah. yep. okay. okay. you need the phase. ooh uh well well it depends on how this i i mean you're talking b yeah, just the carrier. okay, so it's not necessarily phase, but if you're using some sort of sinusoidal coding, it's phase, yeah. okay. okay. w what so what sort of bit rate do you get for the um encoding of the this the hilbert transform? the s the spectrogram. w what sort of coding do you get so far for the no no no no, f f yeah. yeah. yeah, yep. uh wo well, it's no no no, but what sort of bit rate do you get? is it is it uh id do you have any sort of m idea of what sort of band-widths? yeah. 'cause okay, and that's in that's with the fully optimised sort of compression scheme or that's just the raw data or? okay, yeah. yeah. sure, sure. you just cut out m bands and just use interpolation or whatever. yeah. t so that's one point okay, that's with the full spectrum basically being used then. yeah. yep, yep. yep. yep. yeah, yep. okay, that's quite good. so and hynek was showing me how scalable it is in terms of um you can take half the bands away and still get decent performance. you take you take three quarters of them away, it starts to degrade, but it's still intelligible, that sort of thing. yep. okay. and that's without doing any sort of um prediction of what the missing bands might be, right? you just leave them out. i mean you you shou you could be able to do some sort of code missing data sort of bayes stuff, yeah. okay. okay. yeah. it it would be it would be nice to have a scalable scheme though, wouldn't it? like because you've got a nice scalable scheme for the for for the the coding of the the smooth spectrum. it'd be nice to have some sort of scalable coding scheme for the for the voice signal as well, right? well, i don't know, i'm just well, at different bit rates. so the actual uh excitation as well. because i mean it'd be nice uh l let's say if you had twelve hundred bits for the for the sp smooth spectrum and twelve hundred bits for the voice, and then you could scale them both. yeah. sure, sure, sure. yeah. uh no no. just long as it's some sort of non-linear. yep. m the place more emphasis on the yeah, yeah, yeah. okay. yeah. okay. hmm. and then vector quantize or whatever. yeah. hmm. hmm. mm-hmm. so times fifteen. twen that's t it's twenty times fifteen, it's not okay. so it's it's yeah, it's three hundred. it i mean it's it's total of three hundred sort of parameters per second. um well, y you're not you're not carrying transmitting that at all really at this point in time. i uh and and you get whispering type synthesis, yeah. yep. or one of these glottal pulse things. f and just use basic l_p_c_ ten style. yeah, which is which is probably, i mean, if you're going to aim for like a very low rate code to below five hundred bits per second, i guess. that's that's probably fine. but uh the lowest standard is melp i think, which is twenty four hundred, the standard. it's been implemented at twelve hundred f bits per second using more more complicated quantization schemes and stuff like that, i think. i think so, yeah. i s i saw that they used matrix quantisation on the l_p_ uh on the l_s_f_ parameters or whatever. but i mean that's quite it becomes quite computationally intensive. but i mean i imagine the same sort of approach could be taken with yours as well. no. yeah, that's the same. twenty melp*'s a lot better quality though. yeah. ..no no. i think i think i think i think celp or celp or however you pronounce that. it's about forty eight hundred i think. yeah. well the standard's forty eight hundred or something, yeah. yeah. or based around that. yeah. okay. it's not terribly interesting though, really. yeah. so i'm well you you've well, i mean you've got a bunch of options, haven't you. you can use like uh a codec sided or you could use a sinusoidal based. no. yeah, yeah. sure, sure, sure. yeah, but what's more interesting? i i mean 'cause c i i i suspect there somethings like um celp. you're gonna get h lot higher band-width, aren't you. i mean it's a high band-width standard. so yeah. okay. yeah, yeah, yeah. okay. hmm. okay. so i mean how yep. yeah. 'cause it's a bit tricky, isn't it. that's it's not like a standard i'm just thinking, i mean what sort of opt it's not like you can't just do standard um, you know, inverse filtering for um signal reconstruction or anything like that, can you? uh oh okay and then you just then you just do the inverse f_f_t_ or whatever. okay, the oth i mean the other o mm yeah. but that's gonna be susceptible to having phase problems, is that, when you yeah, yeah. yeah. okay. yeah. hmm. and now i mean there is there are um mm there is research uh um pu that's been published showing, you know, speech reconstruction from the essentially the spectrogram without phase information. yeah. so yes. yes, but i mean you can reconstruct the the f the th the entire spectrogram from that, can't you. so you could do it that way. cause i'm just saying there's a heaps of there heaps of options. like if y you be exactly the same as sinusoidal coding or something like that. all you need is the reconstruction of the spectrogram in order to estimate the harmonic amplitudes and yeah. yeah. ye just by refitting. yeah. yeah, yeah. okay. yeah, okay. d it's th yeah. but that's that's the big thing at the moment, isn't it. if if you're mm if i if the if the exci if the if the encoding rate's two kilo-bits per second for the the excitation component of the signal. it doesn't matter how efficient your um encoding of the spectrogram is, you're always gonna have a high relatively high bit rate.. yeah. nah. yeah, yeah. yeah, sure. well, one could argue it's it's a field that's quite a sort of a niche these days, isn't it. um in standards and yeah. mm yeah. well did you say uh who said yeah. is it nokia's being sued in the u_s_ for their tri-band usage 'cause, you know, intellectual property. uh no no, g g_s_m_ because they don't have g_s_m_ in the u_s_. or i didn't have g_s_m_ in the u_s_. uh it's f uh okay. no no n uh well that's tri-band, s yeah. yeah, yeah. yeah. yeah. yeah yep. and apparently nokia's in a bit of trouble over that one, but yeah. well it's interesting. yeah. should've been a lawyer. well that's but i mean coding is a much more sort of it's it's it well i mean it's it's uh it when it boils down to just mathematics to a large extent, doesn't it. it's it's just uh sort of it's, you know, communications s sort of no. yeah. sure, sure. yeah. sure, sure. push to talk and stuff like that. yep. and i mean, as you say, i mean that delay is, i mean you can squash that delay to some extent, right. i n i think as far as the co you know, c c delilah's coding goes, i don't think you can probably do much more than what's already been achieved. because to get higher compression you have to, you know, take advantage of redundancy over longer time spans. so yeah, sure, but i mean that's transmission delay, not algorithmic delay. if you add those together you end up with a the nightmare scenario, i guess. we might have someone else joining our uh meeting soon. oh yeah. i don't know, i mean uh you'd be you tried sinusoidal coding based approaches, didn't you? sinusoidal coding. yeah, but sure. yeah, m yeah, tr yeah it's a yeah, but i mean the th the the thing is you'd need ti it part of the parameter set for that is uh the um uh the f uh the harmonic amplitudes. and then one way of compressing the harmonic amplitudes is just storing the smooth spectrum. and then you can uh and then you can interpolate yeah, you can just pick the the um the harmonic amplitudes on that spectrum when you reconstruct on that smooth version of the spectrum when you reconstruct. y yeah. well, i was just saying yeah. yep. yeah, well i i th it's high. it's quite a high bit rate. but i mean once we it maybe it's something that's scalable. well it i mean you can use as many harmonics as you want. i m h uh he was just using the fundamental frequency. and if you smooth it doing a smooth spectrum representation of the harmonic amplitudes, then you only need to store the the the fundamental frequency and and the and the maximum voice frequency. and everything else can be yeah, s yeah, sure. so, and then but then the tricky the only tricky thing that we hadn't figured out was the phase information. um but y but uh you can still get a like you get th um basically if you do phase unwrapping and all of these other tricks, you end up with some sort of phase representation, which is some sort of monotonically decreasing function. yeah, unwrapped, yeah. so i mean it's something that you could conceivably represent in some sort of low sort of r you know, low bit rate representation, i guess. and then maybe you'd need some sort of correction to um to correct minor phase discontinuities at some point in time. 'cau yeah, yeah, sure. yeah, okay. okay. yeah. yeah. okay. yeah. sure, sure. yeah. yep. yeah, yeah. hmm. no, and you uh you wanna avoid hard choices on voicing decision, yeah. well that's one of the big problems with l_p_c_ ten for instance. i it stinks because this hard yeah. yeah, sure. yeah. yeah, yep. but but that that two hundred this is what they use for like um st they they do use this for um for um pitch marking in speech. so this is what alan black's code uses. but it's very speaker dependent wh on what sort of band width you'd look at this hilbert envelope. depending on the pitch, depending on the person, y you'll get a different amount of smoothing and a certain sort of so i mean, you know, if it's a female speaker for instance you yeah. so it's sorta tricky. yeah. uh n no. y yeah, but from memory that that basic script that they had wasn't wasn't very um effective. i mean maybe if you did some sort of initial first p guess and then and then chose your smoothing factor based on that. 'cause it it you know, selecting at any particular point of what the what the pitch is at the may be inaccurate. but if you estimated it over a second, you should be able to get a decent estimate, i'd i'd think. yeah. well it'd be nice to it would be nice to really see what this what you're a actually trying to reconstruct here. this carrier signal, what what it really looks l yeah. and you did say it looks like a frequency modulated cosine. hmm. yeah. it would have to be 'cause pitch shifts yeah. yeah. yeah. hmm. hmm. mm. yeah. mm-hmm. hmm. yeah, it's like a yeah. and it it does start to get a little bit n non-linear towards the end. but normally that's that's when the components are not actually probably gonna be audible anyway in voiced speech, right. so yeah. yeah. yeah. yeah, sure. c c 'cause any sort of error in that in that line is is transmitted, is realised in a net as a phase error, right. yeah. mm. okay. mm yeah. tricky, yeah. yeah, yeah, yeah. y y y you have s five slices like n across that way, i guess. yeah. hmm. yeah, that makes sense 'cause it's a time domain sort of approach. um and i mean uh and and in in fact i mean it's what you're doing is so different to most approaches to speech coding that it doesn't matter that we don't know much about or haven't had much i it's i mean it's quite different to a lot of the traditional sort uh so uh voi um you know, source filter type approaches or or whatever, yeah. so it's just a matter of minimum distortion time. yeah. sure. unvoiced unvoiced speech is very difficult to b sort of judge on quality compared to voiced, i think. plosives. yeah, plosives are very important though, i guess. so maybe it does a better job of plosives. okay. it'd be nice if we had some demos here. well, i suppose have to wait to your i guess. oh it's a little wa oh bugger. i won't be, yeah. no. you'll practice it before then, won't you? yeah, he's going to oz. yep. i did, yeah. but we're d we're doing a more traditional s s frequency slices rather than temporal based stuff. yeah. well that's w that was one of the approaches, sure. yep. and the other approach was was this h_m_m_ based stuff, which is which is basically just a parameterization of the, you know, the the the the um the source filter type parameters, and then and then learning the, you know, training like a h speech recognition system. but that's not terribly that's got no relationship to this. i it's yeah. yeah, sure. speech coding. i i read some of the other techniques. um little standards. but um i mean i think i think you're on the right track with the analysis by synthesis but that doesn't tell you how what sort of bit rates you're going to achieve or um and because you have to do it separately for each um band, it would be the i imagine that's the tricky part. um because it would be varying quite rapidly, wouldn't it, your um your uh code book. like w within each um when you go along the the the b the one second sort of segment of the band, you yeah, yeah. yeah. well, i mean yeah. first you've gotta choose your approach and not worry about the band-width, and then start thinking about how to given 'cause i mean i guess especially 'cause this is a this is a more of a commercial based project, you've g c you've you've gotta i match the quality before you start worrying about other things, right. i mean d d do they i mean what what what sort of uh uh-huh, okay. well that makes things uh with they're happy. okay. cool. yeah. sure, sure. and at the moment you're at sort of a point where maybe you have to make that choice a little bit. yeah. mm-hmm. yeah. uh which is not yeah. yeah. al although the standards, depending on what standard you're dealing with, sometimes it can be quite ambiguous, right. i mean all the m_ peg standards are basically this is the container, we don't care how you do it, right, but it's gotta fit inside this this sort of this sort of format. okay. and then you have the v and then you have to do m_o_s_ um testing and okay. well, ho it okay. mm. yeah, yeah. well, i th no, no, no. uh no, no, no. yep. well the the what's what's new is in the transmission medium. like you know, new like higher band width transmission or uh or like packet packetized based transmission. and then all th that sort of thing. the where th where they're they're not so much looking at the algorithm, but so much as how to incorporate the algorithm within a new transi mission sort of s medium. basically s like i mean if you d if you see a lot of papers on like um on uh voice over i_p_ or whatever, the the publications are sort of like um how do we deal with packet loss, or or how was it affected by packet loss and and and things like that. yeah, sure. yeah. and optimization of the code and all that sorta stuff. yeah. but even that's less of a big deal now, right, because all the little um i_ s yeah. not really. you could trow matlab on a mobile phone these days. well, maybe not, but yeah. y yeah. yeah, which david did, yeah. it's good, yeah. yeah, we've got all his code implemented. so uh it wasn't perfect. there was little bit of bugs with some of the matching. but it was pretty hard to tell that it was m not not perfect unless you heard the samples sort of next to one another. so that's not too bad. oh, sure.. and i mean it well th the only trick is the phase. and we've got a student starting in january who's gonna start working on on how to actually efficiently parameterize the um the h_m_m_ yeah, yeah. and i think i think we've got a number of ideas for the the spectrum. that's easily enough handled just using standard techniques. but the without phase? i've got no idea. uh you reviewed a paper on a uh steven cox, isn't it? yeah. university of east anglia. it's near c sort of it's it's near the sorta cambridge side of england, i guess. no the much more mathematical, i think. yeah, just basically doing a search through or or stuff stuff like that, don't they? or yeah. yeah. no. yeah, yeah. sure. where you've only got the m_f_c_c_s. but th i think i think the i think it was to was it to do with aurora or something? the okay. i was okay. yeah, but it w it was for say um say you've got someone who has you know, they wanna dispute that this is what they actually said by the recognition. and the recognition network was wrong and you owe me money or whatever. um and then they can say well we've we've reconstructed your speech from the speech recogni input to the speech recognizer. hey voila it sounds like you uh uh i mean obviously there's a lot more to it than that. there's a lot of sort of, well, does the quality stand up to legal sort of requirements or whatever. but um oh no, that there's a separate set of standards i think for speech recognition. y yeah, yeah, yeah. yep. which uh i think requires, you know, that states the un the information they can send is this. so i don't know whether you tack on any extra phase information or whatever. you have to deal with what you're given. so yeah. i don't know. i i it would be interesting. you i mean maybe that's the best approach to look at to begin with is just pretend all you have got is yeah. um because you can also interpolate the pitch information from the the spectrum as well. they do that. yeah. so i mean maybe try to see how good you can go without transmitting any of that information. the pitch was really good, i thought. i th didn't think you could really tell. yeah. yeah. yeah, but but the thing is if it's mel scaled, if you have a mel scale, you get quite fine reconstruction of the spectrum at the lower frequencies it which is what you need for reconstruction of the pitch anyway. so i mean if you wanted to transmit with fewer bands, you'd maybe still keep the lower bands or or maybe you could still transmit the pitch. the pitch isn't that big a deal. but it'd be nice if you didn't because then i mean that's well, i mean it's worth giving it a go and then men maybe seeing wh what information you can add to the data stream to improve performance. um i don't know. that that'd be cool. so get coding. mm i think w ah, well i mean huffman's easy enough. but but yeah, sure. uh that that's where i stop. run length encoding, i'm out of here. okay. i mean i i th i think they do try to i mean obviously the the the techniques they they choose are supposed to be less, you know, susceptible to coding errors, let's say. you know, i but i mean that's maybe the the limit to that. yeah. okay. okay.
yeah, at least on paper. yeah. yeah, at least you can write on the p yeah, celp. yeah. hmm. no, but marius is it's similar stuff. no, marius is also using like hilbert transform and then both. yeah.. uh is some kind of d_c_t_ or again yeah. yeah, ok yeah. yeah, yeah. yeah. how c mm-hmm. yeah. because use linear phase or yeah uh oh okay. yeah. yeah. same frequency. yeah. for l_s_f_. main bit rate. what is good. oh. ah, okay. hmm. hmm. s yeah. hmm. hmm. yeah, yeah. hmm. oh yeah, type. hmm. s hmm. s so oh okay. smoothed? or like apply more hmm. ah. hmm. so after you're doing l_s_f_s so or like i mean how you're finally hand coding ah. oh. yeah. so first you apply these triangle filters and then do l_p_c_ coding or like yeah. yeah, okay. yeah. filters. yeah. oh okay. hmm. you can do the l_p_ analysis on each sub-band, yeah. yeah, m yeah, ok yeah. yeah. yeah. yeah. yeah. so how many l_s_f_s you're using and oh okay. so but you how many bands you're using? if you're you're not skipping any bands or like oh ok fifteen into twenty then. like uh. hmm. ah. oh yeah. but phase you're not encoding anything. phase you're directly transmitting that. uh no th yeah, yeah, yeah. the phase of hilbert envelope. you're not transmitting at all like. yeah yeah, is noise. fourier. yeah. yeah. so what is the celp standard now? like it's it's two kilo-bits per second? or the oh that two two kilo-bits per t ah ok ah. some yeah, yeah. but yeah. hmm. yeah. but this is codec centred, no. you have code book. so oh it's not codecs code book. there is not code book. oh ok oh. yeah, celp is yeah, it's it's it's around one kilo-byte like. yeah. yeah, that's a, i think, even g_s_m_ and all this, g_ seven two nine. yeah, yeah. on based on celp. at least it gives but still they use a code book, no. so that's really uh it's maybe better than like using just random noise or yeah. yeah. yeah. yeah. huh. yeah. ah, okay. hmm. but hmm.. inverse, yeah. inverse hilbert. yeah. yeah. or output signal like yeah. and then yeah. yeah yeah. hmm. oh ok hmm. hmm. hmm. hmm. but yeah but it's more towards industry, no. like the yeah, because i worked in a industry for like a small time. but they're they used to implement all these coders, g_s_m_ and i_t_u_ standards. no, just before my p_h_d_. for like few months. yeah, but that's uh like industry it's really a big deal like. so they they they work on a lot of these standards. yeah. but mostly they're all based on celp like mainly. yeah. so how? tri-band is g_s_m_. c_d_m_a_. no no, but c_d_m_ is there but now slowly they're introducing g_s_m_ also now in u_s_ like. yeah, g_s_m_ tri-band. like the frequencies are different, like very uh ah. yeah. now it's really big competition. yeah. standard maybe like established. yeah. so where do you think your thing is useful? in c_d_m_a_ or g_s_m_? i think maybe in c_d_m_a_ it may be m c_d_m_a_ is more like more time domain. the they they are c doing only yeah. yeah. yeah. yeah. but you know really the these long line telephones, they already have delay, no. at least half a second or yeah. yeah. yeah. yeah. but mobile m and this one is uh harmonics and the yeah. and then picking yeah. yeah. but maybe the bit rate is the problem, no. like what david was doing. like it's really uh yeah, yeah, yeah. yeah, maybe you can just see how many how many harmonics. he was using twenty, i think. like yeah, maybe you can try to reduce. yeah. yeah. and the f and then all this amplitude. ah. yeah, yeah. yeah. yeah. each again. some kind of linear yeah. yeah. yeah. some function or like a hmm. yeah. yeah. oh. yeah. the size of the but you don't do any voice un-voice decision, no? yeah, the that's even advantage i think for this approach. yeah. yeah. yeah. sorta the modelling, it's really another issue like. yeah. yeah. yeah, yeah. yeah. yeah. and then you have to combine all this harmonic model and noise model. so again you may end up some. 'cause oh okay. the pe peak uh yeah, yeah. yeah. yeah. s yeah. yeah. so that's a you have to tune lot of parameters like i am. but anyway, you don't really need hard decisions, no? like voiced unvoiced. so m maybe it may help you, but it yeah. yeah. yeah yeah. yeah. but do you really really need it or like voiced unvoiced? yeah, but yeah. yeah. but it's not really you need t you don't need to specify at each point it's voiced or yeah. huh. yeah. and then amplitude. but you're telling even frequency's also shifting, no? like your delay. yeah. yeah, yeah, yeah. yeah. so then, yeah, then it's not exactly amplitude modulations or it's it's in between maybe. yeah. s yeah. so yeah. yeah.. that's what marius did, no, like. but but he was keeping all the phase information. no, but even the his coded like he just he he didn't do anything, he just transferred it that way. he was aiming for he's not really kind of doing speech coding, but he want to do it on music. so he doesn't really care about the band-widths or he want to have the quality, no, like. yeah, but even especially the audio series and the that that's what he was telling like. yeah, at least even ten k_ bit. twenty kilo-bits also is okay. so but he's uh he's not i think finished with all that thing. he's just transmitting all the phase and then but he's encoding the envelope, yeah. yeah. yeah. yeah. and then leaving the phase, yeah. yeah. yeah. hmm. yeah. yeah. yeah. yeah, or auto-correlation coefficient and yeah, they again give the phase. yeah, again give the phase. yeah. oh. hmm. but this phase is different from your carrier, no? like yeah. because you're just doing on uh your uh yeah. and also like yeah. and yeah. hmm. but even you can use as a can you use that phase directly? whereas the d yeah, this one is direct but this is phase of your d uh spectrum, no? like it's not in time domain. so of your signal. it's not so then it's bit tricky. like how okay, but still you get only the the magnitude of your hilbert envelope. you won't get the no, but you started with hil hilbert envelope, no? like magnitude of hilbert envelope. magnitude of uh no, before d_c_t_ your d oh. yeah. ah, ok so input to the d_c_t_ is the speech or like your yeah, okay. just you're starting with ah. ah. ok ok ok yeah, maybe the then yeah. ah okay. yeah, it's in frequency domain, no. yeah, definitely it's d but now i think people yeah. yeah. so when is your turn? uh oh. oh okay. i mean just before. oh you.. low bit rate. yes. yeah. hmm. hmm. hmm. oh okay. yeah. yeah. yeah yeah. yeah, you know, the the yeah. hmm. yeah. uh. no no no, yeah. yeah. but it's mostly like this is uh s celp and these thing. no, i was t uh doing some uh like writing some code for testing and these things where i'm making some t standards for that. so so th th there are like li the thing, these standards, how it works is like they give all the pseudo code kind of thing and then they put severe test conditions. so you have to pass those test conditions. like then j you can say okay, we implemented d_ seventy eight or d_ se yeah. yeah. yeah. yeah. yeah. yeah. yeah. so that's why it's really even the t standards they give the test signals also. you are t you can't really do it on whatever database you have. so so you are to use the uh uh yeah. uh it yeah, it's more like uh they define. so i did for some even also. they're also they define all this the speech condition. everything they define. so you need to follow all those. yes, this industry is again different like, yeah. yeah, i think there isn't only se most yeah. ah. celp is from uh a_t_ and t_, no? like a_t_ and yeah, yeah. it the all this l_p_c_ stuff is yeah. ah, ok ah, okay. yeah. yeah. yeah. the yeah, that's the main thing even. yeah. yeah. yeah. yeah, like i. voice over i_p_ or yeah. uh yeah. yeah. yeah. yeah. and again, lot of this stuff like whe even when i was working mostly that company was developing for voice over i_p_. so they're to put all these things on again d_s_p_s. and also it's mostly like how to how many l_p_c_s are yeah, how many co yeah, it's optimation and then it's even yeah. again, now now now even they're also becoming yeah. even the memories are not really yeah. yeah. yeah. yeah. so even i think uh then you can have whatever band-width is. it's short really. but no, i think uh but still then that's what maybe people are going toward sign sort of modelling, no. like i was before when the bandwidth conditions were really stringent then they were using all the p l_p_c_ based source filter. yeah. yeah. yeah, if you want you can look at his code and. but only th yeah, yeah. yeah. yeah. but definitely it's better than co coded speech, no? different. yeah, it's really but uh how much you can get the compress these like what you're telling is you can do all these tricks like. yeah, these yeah. in yeah. well yeah, i read uh i reviewed one paper like yeah, the that's that's from no. yeah, maybe you can look at dates from uh you know east of uh east uh yeah, east anglia. yeah. uh this uh e east university of east anglia, ben milner. yeah. i yeah, it's yeah, i can forward you that. they're doing lot of tricks. they're like they're again training gaussian mixtures for different frequencies and then but they can they're following the yeah. yeah. yeah. no no, the those things are again based on l_p_c_, no? like mainly like yeah. whereas like the pitch estimation in in the x_ waves or these things, they're all dynamic programming based on again l_p_c_ analysis. yeah. and then construct that. huh. yeah, again like uh no, the problem is different for everybody. like these guys, what they're doing for distributor speech recognition mainly, this uh so. uh yeah, m_f_c_c_s. so no, it's e_t_s_a_ or something. european standard yeah hynek really n yeah, even it's h kind of aurora framework. so yeah. no, but now it's bit. yeah, now they're and then yeah, m_f_c_ s but even at distant point they're planning to use for j uh standard d_s_m_ or d_s_ it's not? or yeah. e_ uh e_t_s_a_i_, yeah. european, yeah, commission. so m yeah. yeah. yeah. yeah. yeah, i yeah, all these standards are really okay. yeah. but yeah. but the quality again problem, no like? no no. the the eu again, european standard, what this give us, they give the m_f_c_c_s and they give the pitch. then you are to construct the thing. but if have m_f_c_c_s then like again you have to get the high order m_f_c_c_s and then yeah, then you're have to yeah. yeah. yeah. hmm. mm. yeah. but i think again it's different, no? like these standards again, all these g_s_m_ standards. maybe they have the compression standards and again coding stand but maybe uh i i read ah yeah. i don't remember anything like yeah, they're doing yeah, combi yeah, the even even g_ even normal g_ seventy nine, normal speech coder. so they have the speech coding standards again. they are these channel coding. so yeah. so it's mm. yeah, b maybe. yeah. yeah. again, yeah, i so okay? yep. th thank you then. oh.
you can write on the paper, yeah. it's it's okay. right. right, right. mm-hmm. hmm. okay. i have a question. is the scale linear or is mel scale, like in m_f_c_c_? is it bar scale, like in like in sp yeah, okay. the overlapping is different, right? yep. mm. okay and? okay. uh-uh. mm-hmm. okay. mm-hmm. i have never done anything. yeah, tri-band g_s_m_, yeah. different frequency. yeah, exactly. mm-hmm. okay. true. mm. mm-hmm. i've never done any coding in my life. so i just have one curiosity. so this is coding then s you code the speech, then you're also supposed to transmit the speech, right? th let's say in g_s_m_ or something else. i remember when i was doing some image processing, there were techniques for compressing and then coding. so adding channel information on the image at the same time in order to save sound bits. you see what i mean? exactly. i d was just wondering if there's some s someone was doing it with speech as well. well actually, when you compress yeah, all like first you compress the signal, right? and then you al you add com some complexity to the signal in order to recover the error. so i was wondering if by any chance there was something like this in speech with the speech. you don't know? okay. well it's not the most difficult, right? sure, sure. and and this was the problem actually because you were no not not in g_s_m_, definitely definitely not. uh okay. then th they some, you know, source. so yeah. okay. just one. yeah. definitely. is it good? okay. okay, thank you.
oh my god. yeah.. okay. i should start? yeah, no so i thought that we might s t speak a little bit about speech coding because nobody is doing here speech coding and i would like to have some maybe feedback because except me. i've i've supposed to some speech coding stuff here with hynek uh but you know that little bit at least that uh based on some let's say hilbert transforms using a longer temporal context and deriving some parameters would be tranf transcs transmitted to a decoder and so it's a little bit different than the stuff which people are using now, which is like a l_p_c_ based. and but we are at the beginning with everything more or less. so still um we can't uh d encode a speech. but of course there is there are many problems which we still didn't solve and the biggest one is quite similar to l_p_c_ stuff where people still don't know how to exactly encode a carrier or the source signal. we know what to do with the envelope or how to approximate the envelope, let's say. um when you have got oh you mean that it would be nice to plot at least s um maybe i i can i can so what we are doing now, yeah, you mean. okay, i uh everybody knows l_p_c_ stuff, how it works. so i don't have to present anything mores. but it's uh m more or less very simple. we have let's say at the beginning there is some speech signal. what we are uh we might see it uh in very simple way that we are using some hilbert transform h at the beginning to create analytic signal. the analytic signal is the signal which is a complex which you can create from the uh real uh uh some sequence. and if you get if you apply hilbert transform on this, on such a speech signal, like a real trajectory, then you get a complex signal and if you take just the amplitude, then you get hilbert envelope code. so i well i i don't know. yeah, let's say ups. yeah, it's u it's not p_l_p_ at all, d mm. no he's it's pretty much the same. uh you're lit i know what you mean with the pu p_l_p_. they are applying p_l_p_ with some iteration with this this one approach. so they are doing some smoothing of a spectrogram in two ways. one was right. right. but p_l_p_ is using just the spectral uh direction. let's say you are doing uh for every twenty millisecond processing one one feature vector. here you are processing one spectral band, frequency band, right. so it's different direction which we are w using. but anyway, this is very simple. so applying that uh absolute uh absolute operation, you get a hilbert envelope. and of course then there is the phase. so it's just the complex operation. and that's more or less all what we are doing. so now what we know what we know is how to approximate that hilbert envelope. we are ap we are applying that hilbert uh him what is the marius' uh stuff. so he knows how to encode uh the hilbert uh envelope using like l_s_f_s or l_p_c_. you are applying again to some linear prediction model after that. but it's uh right, it's in a it's a little bit difference in not applied in the frequency domain. so instead of classical l_p_c_ domain, where you have a spectrum, let's say something like that, and you are applying some linear prediction model which is trying to approximate the spectrum, right. so this is frequency. in our case we are we are trying to approximate s kind of instantaneous energy. so if this is a time, and you get one and add one frequency band, you can see something like this. and we are trying to approximate that envelope which is that hilbert envelope. right? so again, we know what to more or less we know what to do with the hilbert envelope. we know that we can apply linear prediction model and it works pretty well. but i if you want to really construct the speech signal or the you need also the phase signal, which is in uh some sense quite simple for voiced speech signal, say. but s uh the phase no, you ac it's a phase or you can encode it like a carrier. so the carrier is, if you know, uh amplitude modulation, right? you may see something like there is one cosine which is uh very would have quite a f right. yeah. it's called like the phase. right, right. exactly. so the this is the signal which you also have to somehow encode or transmit from a code of the decoder in order to re-synthesize it, right. so what do you mean? again. for this one? for the the linear prediction model? well it's uh just a linear prediction. classical way. i mean oh, okay, w well, if you want to see it in more details, i mean oh, you mean uh yeah, bit bit rate you mean? okay, yeah. sure, sure. so whereas we are what we are now is, let's say, something like one point two kilo-bits per second for uh that hilbert envelope. is it's again a roughly estimated somehow. but we still know that we can go down and right. exactly, yeah. i don we are using like fifteen bands. right. but they are first they are inter oh well, integrated into fifteen bands using oh classical way like in m_f_c_c_s or in p_l_p_. you just apply some those uh triangular windows or something like that in order to to get a li really like just fifteen bands, right. and each band is then processed independently. so for one band we more or less have w like w one hundred bits per second. for hilbert envelope. right. right, right. you yeah, you can just right. right. right. yeah. uh right, that's what we did. yeah, sure. sure, sure. yeah. there are many things we would wouldn't at all. but yeah, exactly. but now what i r really trying to do is somehow approximate that uh carrier signal, to do something with the carrier. there are many simple things which you can do like uh quantization quantization, for example. right. you mean something like uh f uh that celp coding system where you're or wh wh wh what is no, what is exactly scalable system? what you mean by that? oh yeah, yeah, sure. this was yeah. for dif yeah, right. right, right. yeah. but i think this appr approach can do that because you really can split it into several bands. and you can decide how many bands you want more or less. so yeah, this is a bar scale we are using. yeah, or mel mel scale, so it's uh it's being uh more wider for higher frequencies and of course the s the spaces right. yeah. but what i w my sense is that it is not so much important still. i mean it doesn't matter if you're als use mel scale or bar scale. but right, yeah. but that's what we are using, yeah. uh actually we are using just gaussian windows here instead of those triangulars because they are more uh no, it's better than for decoding it's better to play with them because they they are not zero anywhere, you know. they are they are approaching or in they are in zero. but actually they are not. so d you don't have to care about some zero values here and but more or less it doesn't matter, yeah. we are using just gaus gaussian windows. okay, so l those are that the hilbert envelope is um is approximated by linear prediction. so you have l_p_c_ coefficients which you can transcribe into l_ l_ l_s_f_s, which are right, exactly. yeah. right, exactly. so first uh the approach is quite sim simple. so at the beginning we apply d_c_t_ in order to get to frequency domain. then you apply those uh triangular or gaussian windows. so this is done for one second, eight thousand samples for eight kilo-hertz sampling frequency, right? so for such d_c_t_ trajectory you apply those uh gaussian windows. and then i so you split it in two frequency sub-bands. and then each one f frequency sub-band is more or less uh right. right. exactly. you just apply l_ l_p_, so auto-correlation let's say, or you can do power spectrum, whatever. yeah, so you for each one you get l_s_f_s here. uh well, i'm using now like twenty l_s_f_s per one second, per one frequency band. fifteen. no no. if i just keep all them so twenty l_s_f_s, each one is um is quantized by four or five bits. of what? three hundred bits? two. yeah, yeah. yeah. yeah. exactly. you mean this phase, the the hilbert carrier? or no, of course we did yeah. uh w well, there are many things you can do. we don't well the first thing what you can really do is just to replace it by gaussian noise. so you can really use t like in uh l_p_c_ system. right. exactly. or you can replace it by some cosine, one cosine generated in a cen centre frequency of that gaussian window. right. yeah. it mi it mi yeah. twelve hundred? yeah. yeah, it's possible, some of it. yeah. mm-hmm. r right. well i'm just comparing with uh l_p_c_ ten standard, which is like classical l_p_c_. a little bit optimized. so there in the there is i think two point four kilo-bits per second, the bit rate for that. yeah, definitely. this is no, uh i mean l_p_c_ ten is just l_p_c_. no no no. no. just noise or just those impulses. celp. yeah there yeah, yeah. yeah. yeah, yeah. four point eight or something. right. yeah, sure. well more or less what we didn't do is analysis by synthesis, right. so that's what celp is doing, right. so yeah, so this is what we do and uh that might help a lot, of course. but w yeah, exactly. that's that's the question. it will be. yeah, yeah. yeah, sure. for example, uh there what i also did was uh a simple peak peaking algorithm. so if you take a look on that spectrum of that of that carrier signal, which is more or less like the cosine if you really take a look on that. but it is kind of frequency shifted cosine or frequency modulated cosine. right, so it's th the frequency still to be changing. or of course the amplitude is just not constant all time. it should be theoretically. but practically it's not. so what you can see for a one second signal, you m more or less you already might see some like one one uh impulse there or spectral line for. but usually there are many others which are s quite small, but they are quite important. if you don't transmit them, then the signal is quite robotic or something like that. so if you're really transmitting just one spectral line. but this is what you can do. and you can understand pretty well. the signal is audible. or if you're adjust uh th find one one line, one speaker and dom dom dominant one in the spectrum of that c uh carrier hil hilbert carrier, and you just transmit this parameter. no, n it's even easier because what you have to do is just divide the you just multiply that hilbert envelope by the table carrier. that that's all. yeah, yeah. yeah, exactly. so so you just multiply that uh one hilbert carrier, which is transcribed by l_s_f_s by that this one i_f_f_t_ signal. so oh f you mean the phase of that uh of that carrier? of of course i try to play also with that, and uh i found that it doesn't matter so much which what is the phase because we are using just one second window, right. so of course there there might be uh some uh disturb uh disturbed uh when you concatenate those segments together. but then the phase might be important. but otherwise, i mean, i didn't find any anything important with the phase. like great, keep the phase zero and it works. or right. right right right. yeah. but it's again a little bit different, because they are i they are doing this in a frequency domain. but this is in time domain, more or less. right. the spectrogram, you mean. yeah, yeah, yeah. yeah. right. right. exactly. what we can also do very simple is just to get the magnitude spectrogram using this approximation without the carrier, just hilbert envelope. but you just uh get it in in this way, in in the way of rob's, right. but you can also get the s short time fourier transform uh and keep it right. just classical uh frame by frame algorithm, twenty milliseconds. uh but you keep just the phases instead of magnitudes. and you can put this together, right. and it works pretty well, and then the signal is not uh just uh whispered. then you really hear that there is the voiceness in the signal. but uh of course the quality is uh is not so good still. but uh you can use it for like two, three kilo-bits per second and it works. like yeah. right. right. yeah, sure. anyway, so um hmm m maybe it would be nice if you t if you tell us uh what you do with the speech coding 'cause i d i didn't see any of p_h_d_s of you, i think. so if you did something so maybe it would be interesting at least to yeah. yeah, sure. it's true. so you were when when did you do that? oh okay. uh-huh. i think so, yeah. you mean using that uh c_d_m_a_? or yeah, it's not g_s_m_. right it's uh kind of s no c_d_m_a_ is uh another level, i think. mm-hmm. i think there is a l lot of money in that like, right. like speech recognition, of course it's very interesting, but people don't use it so much as s just coding, just telephoning, right, or just right. yeah. they s yeah, yeah. right. right. no, i i. well what we were what we were thinking about is oh, c_d_m_a_, i mean there is the b well, the problem is this in with this application, or it doesn't have to be problem but one constraint is that we are processing one second of the signal. you can go down of course. but then you are losing little bit of the bit rate, let's say, right. so but there are many applications where you really can use it because you don't care about algorithmic delay, one second algorithmic delay, like for i don't know if instead of s_m_s_ you can really have some channel where you just right. um so probably. right, right. yeah. right. yep. right. what if s which one, s oh well, but the sinusoidal coding is usually m when you s at least what i think or uh what i know is when you are doing sinusoidal coding it means you just take the speech signal at the beginning, right. and you are trying to find to uh yeah, to find the harmonics in a temporal domain, right, for that. right. oh. yeah. right. more or less i did it maybe different way, but kind of algorithm like okay, trying to find out the strongest or the the most dominant peaks in the spectrum, which is more or less the same. but th but you know, the the f the interesting right. right, and that that's it. and then you do multiples of right. yeah. yeah. like it if it is unwrap you mean. unwrap uh phase. right. you l right, yeah. yeah, yeah. i also play with that. and for example what i found i well, in this case uh we are not trying to encode the speech signal. we are trying to encode some carrier, which is much easier, i would say, because you can really im you can see that even. so what i was trying to encode at the phases, so i just found that uh two bits is enough for that and you really don't hear the difference. so if you keep all the spectral lines as they were there but you were just uh quantizing with two bits or three bits for amplitude, two bits for for phase. so it's okay. the problem is that you have to transmit many of them. 'cause there is not just one dominant component but usually more others which are quite small, right. or what you have to do then you have to decrease the the the size of not one second, but you have to go down because then you can get just voiced part and unvoiced part. you know, one second it's usually mixed right somehow together. not now, no. i wi uh uh we don't no want to do that, yeah, because it's quite tricky. right. yeah, sure. exactly. right. but for example what we also found is that if you use just the hilbert envelope, that that one which is trying to approximate that energy in some low band, like ar around two hundred t uh hertz, and you take a look on the signal, on the trajectory, you can see that it works like kind of um voice detector. because uh for a low energies there is usually noise. for high energies there is kind of a harmonicity. so it works pretty well for that. so we might use this one information for that. but yeah, i think so, yeah. mm-hmm. right. uh-huh. mm-hmm. mm-hmm. that's true, yeah. yeah. yeah, yeah. yeah, that's that's a problem. sure. uh f that would be nice n not to have anything, yeah. right. yeah, yeah. my mm-hmm. right. exactly. well i don't know. maybe if you we uh really want to improve the the quality, then we will need it, i don't know, still. n not now. yeah, uh well, i'm gonna t have a time next next month or in three weeks so i'll try to play some examples there for you and what we are trying to reconstr how does it look like or sure. yeah, more or less. it's kind of uh not frequency. let's say amplitude modulation when you have ju just the carrier and that modulation signal, right. yeah, we yeah, yeah, yeah, exactly. yeah, yeah. it's in between, exactly. so it's not this and that. but still yeah. but anyway what also the other approach which is possible to use. the other is one to keep the phase like this one, like two spectrograms, one the phase and one the magnitude. and well i can again play it and it works on two kilo-bits per second. quite what uh no, what uh he was doing he was using it for speech recognition, what i what i know, but for speech coding no no no, it was i don't know. for speech coding you mean? i don't know. i don't know. m maybe. right. yeah. mm-hmm. well for example this phase information right. yeah, yeah. i think so. there are m more papers which are using this approach, like trying to encode the magnitude spectrum or spectrogram. uh the phase or you just uh re can again the phase can be replaced by the noise, which is pretty good. so that's uh the benefit uh variable approach. so if you really don't have a band width, you just can replace it by noise and it works. of course then it doesn't sound as the original because it's more whispered or whatever. but you can understand well. and for example uh if you mention this one approach with the that uh trajectory of the phase, of unwrap phase recre uh i also use because you can do it here when you get into uh this domain where you apply uh or you're trying to compute uh s power spectrum in in order t let's say that this kind of trajectory, it doesn't in fit the same frequency over time domain. but here you are trying to compute uh doing a to do linear prediction. so we are just doing f_f_t_, right. then applying uh power in order to get um to s to power domain, and then i_f_f_t_, right. so you get the auto-correlation coefficients. so before you're applying uh power spectrum, you can again get the you have got a complex f freq frequency or complex spectrum, you can get the phase, right. and the phase look quite similar like this. so you might see it's is more less like a line. right. yeah. right. yeah, it's it is very different, yeah. then you are doing in different domain. i think this is in frequency domain, this in time domain. um it is quite it's quite sensible to any quantization, what i found. really it looks like the lines. so you just say okay, let's p let's put the line there. but it doesn't work. re well, you can understand. but you can here that there are many artifacts which right, exactly. right, right. and the another problem is that uh it's not band width variable. so you cannot say that this part will be the just the noise, which you can do in here in this this other approach. so that's why i didn't play with that so m more because i found there are more uh disadvantages than the advantages, this approach. so that's why. you mean this one? yeah, if you take just the original phase and the right. yeah, it's it's it's not a phase. yeah, yeah, yeah. it's the phase no, you can again put the that envelope with this phase together because envelope is twe no no. you g no, you get the f yeah, you get the complex complex spectrum with this. no, no, i started with the d_c_t_ no, after d_c_t_ you are trying this is so kind of again a real trajectory, right. and you're trying to first find that hilbert envelope. so uh let's well so this is a speech. no, but here what i want to say is really that y it's not part of hilbert envelope, it's part of the d_c_t_ signal. so it's not it's enough if you have just this phase and f and the hilbert envelope, you can put it together. you don't need the carrier. i mean this is kind of a carrier signal but in different domain represented. right, right. so i what i think is that we already will need to apply s kind of analysis by synthesis approach mm uh in order to get something. right. what i no. you mean ye right. right, right. yes, that's true. but i mean that's maybe even the advantage here because you it's not speech d based, which means we are not right. so you can when i tried and comparing with l_p_c_ ten on a uh twice bigger or higher bit rate, this work works better, it seems to me. just for unvoiced speech. o i'm not mentioning uh voiceless because that's unvoiced, which means whispered. uh yeah, but uh when you hear and try to use some examples with the music for example right, yeah. mm-hmm. that's true, yeah. uh but anyway, it seemed to me pretty good. i mean that there might be. that's true, yeah. uh it should be uh thirteenth december i think. i just uh you won't be here. uh okay. no, i just swapped it with uh mike perrot i think 'cause h he is not going to be here this time. and i was supposed to have it in uh in january. so whatever. anyway. so but anyway, i i wanted to hear something maybe from you what you are doing f with speech coding because you were you mentioned that by but it was based on like you were using kind of recognizer there, right? uh-huh. right. right. yeah, it's i'm i mean yeah, it's different. but uh it's speech coding at least. i think so. right. yeah, sure. band you mean? right. with different i don't know. but maybe what we can already do is uh to decrease this uh this distance. we can do a hilbert envelope approximation with one second. but then we can split that signal into, i don't know, ten ten uh segments with the one hundred milliseconds or even less, and we can apply some other technique for that, you know. still right. exactly, exactly. yeah. well, th more less they don't care what's uh what will be the quality of the of the or the bit rate, they just want to get something new, you know. so if you get the v very uh low bit rate with a reasonable quality or really high bit rate, which is c again different, but with uh high quality, uh they would be happy with the both. so you know, on one side it's pretty good but on the other you don't know which way you should go 'cause there are many of them. and uh right, right. okay. well i i will w uh do you want to mention your speech coding experiments? so you are saying that you're doing something i in india with uh so you know the sub mm like um more details? oh yeah. okay. right, right. right. i see. yeah, but it's quite interesting with the even with th uh s current speech coding uh technologies. we are they are still trying to use k zap or those r_p_ based approaches. there is nothing new, more or less. and if i when i was uh listening the uh the talk of uh milan jelinek, uh he's the czech guy, but he lives in uh in canada. i don't know the name of the university. but they they have uh the pattern i think for celp even. or um i don't know how it is exactly but they got very very famous for this for that celp. what they did exactly i don't know, if something different a little bit or not, or they just keep those patterns or i don't know how it is exactly. maybe hynek will know much more. but uh when i heard those that presentation, they didn't do so much new. i mean uh it was last year and uh i think hynek, he was asking wh so what's new that you are so famous with that and they n they said okay, we we know exactly how to do that. i mean you know, not to make any errors like in because everybody knows how to do that. but uh it's not so easy to implement it, right. so uh there is nothing so much new really like. yeah. right. right. exactly. right. right, ri right, exactly, exactly. but then they are s still using like error signal, which is going to be approximated by code book, right? and a linear prediction, which is use which is approximating just the spectrum of for for and everything that that's that's all. so still right. yeah, yeah. right. well, i hope that we still have a a lot of time for that because uh i think yeah, it's pretty interesting, yeah. for example when i was uh you know, harmonic voice modelling, that h_m_m_ system which is using used for synthesis more less. it's pretty interesting. i mean it it works it works well. right. right. yeah, yeah, it is. you mean uh in sinusoidal uh modelling? yeah. and but anyway, do you know exactly how it is done when you really have those m_f_c_c_ coefficients, which means magnitude spectrum, and then you are trying to reconstruct the speech? without a phase. because i i tried you know those th those papers. but s i'm little confused about it. i don't know how it works. but which one? yeah, that would be nice. oh. you know, m but there are some algorithms which were quite uh yeah, yeah, yeah. yeah. uh it seems to me, yeah, something like that. like try you know what you can do um maybe. what you can do is uh take just the minimum phase of which you can get from a linear prediction, right? you know what i mean, minimum phase signal. and it sounds quite uh reasonably the two. but of course they are li it's not original at all. so i don't know how they do that exactly. no, i've no idea. right. true, right. for n for aurora we didn't do anything with that. yeah, but we didn't do any speech recon re reconstruction. yeah. yeah, yeah, sure. right. right. right. right. you mean with this phase uh sure. yeah. yeah, they do some kind of reconstruction of speech. uh yeah. right. that's true, yeah. right. yeah. yeah. so yep. yep. mm-hmm. mm-hmm. sure, yeah. okay. so how about you? yeah. yeah. you mean like channel and source coding together? or no. well i can imagine kind of like j_ peg operations, which might be more less like it's source coding and then we also channel coding because we just smooth the or just i don't know. i don't know. right. right. right. r right, right. i don't know. i don't know so much about s channel coding, like those huffman coding stuff and, you know. i mean it's just standard. yeah. they're just standard techniques. but yeah, exactly. yeah, yeah. i think so. h yeah. end. okay. thank you guys. uh if you find some good paper which you might there was that fam fam waveform pro prototyping cor
