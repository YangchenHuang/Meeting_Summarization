mm-hmm. actually uh you can t perhaps just uh just th there is one frequency that is the base from for uh okay. both. mm the pitch and uh and the freque okay, s and you can measure measure that? or okay. and i is it uh expensive to do, but in fact if y you speak of uh a lot of things that are not expensive, at the end you have uh something that is. hmm. so you have to do f_f_t_, because otherwise mm-hmm. yes. yes, but it i it it is im it i it yes, it is important for detecting patterns. because uh if you say uh let's say uh w how much the person um have spoken over the the five last minutes, uh uh if someone come and yes. yes, yes. y this is uh mm. okay. so i uh this is only based on uh you give uh the spectrum and uh uh okay. mm-hmm. yes. but uh the last ones are always the mm not really. mm yes. hmm uh i i said uh we want t to know to um to do separation just because of the voice recognition. i mm this is not the yes. yes, no, but but in fact we have uh yes, we are designing uh a noise sensitive table just f from your ring, but we are also designing uh a prototype um to analyse the conversations. so uh we might add there a lot of uh features. okay. and uh i don't know, perhaps uh perhaps if you have two people speaking uh speaking uh at the same time, uh i don't know if uh voice recognition of an uh such a stream is uh real. okay. yes, so that's fine. mm-hmm. sure. yes, it's not um every important feature for now, but mm-hmm. yes. and uh uh another what uh. um i don't remember. yes, also uh to to extract the context. mm because if you know the words that are spoken, you can let's say uh someone say uh he's angry uh about the word he's seen. yes. you mi you might want to add uh this kind of this is not in uh my project, but yes. um yes. and have a special events trigger if sure. sure. no, it's it doesn't sound simple, but yes, you can yes. sure. yes, in fact uh for now we are using uh a kind of calibration, but uh it's still um yes. yes, um uh but in fact you have to sit in front of the microphone, and it's uh yeah, people one speak and people two and people three and you are just uh subtracting the the level that you you kept in the other microphones. okay. okay. mm-hmm. mm-hmm. okay, just uh a last question, um when you mentioned it. um if i'm speaking very uh, how do you say it, um yes, i'm just speaking one word every uh every one second. yes a. uh is the the event that is detected is um will be uh a sum of uh small parts? there won't be uh okay. so you yes, you have to to reprocess to uh to say let's say yes. there is a very small um interval. you have to consider it uh. because this is yes, this is one this can be considered as one interv intervention. it might be. this is not a very uh easy problem. i'm done. sure, yes. and you can code java. you can choose matlab. yes, sure. you have to code it uh directly to the machine. hmm. my sheet is very uh filled up. i will take yours.
okay. mm-hmm. or actually look at the pitch, no? don't ask me how it's done i i know the theory, so, yeah. isn't it the pitch and the frequency? the base frequency of the and if you have both you can um well, you can compare this to to one stored and so get out uh which person it is. pardon? and uh well, the things that make the sou the voice. yeah. and and the the holes in your head, everything, the the nose and so on, and that's that's really uh unique for ev well almost unique for every person, so. well once you get this information, your frequency and uh your vocal contract or how it's called, uh y you can use this information to compare to to to the frames here. no, but no no no no. that that that everything is done through through the voice here, you take a voice of someone, and then you can get out this information, it's it's not p it's clear it's not perfect, but s it's enough to to to use it for for different purposes and uh um not not that, well uh as far as i know. actual is it called p_c_m_? i thi um well it's it's not a technology s yeah. it's more or less used this to compress audio. i have to i want to check it, i can show it you afterwards. well i did a course about it, but uh i shouldn't say it too loud, because um the professor is working here at idiap. i should reread what i signed there so. well uh it i'm even not sure if it's that important to know which person is talking, because uh if uh if it i think about a table, or so something uh physical, uh you just want to know in in which corner of the table uh speech is coming and not uh which person produced the speech, so. yeah, but it's not really related to the ah, yeah, i see it's uh you need a bit both uh sound. you might might want to do. you need to separate it. no, but i think once you have the voice for a person, you can start to do this and uh once uh people have speech recognition or uh far enough to t mm uh. i think so. the link. mm. okay. no, i think does this work, email? does it work if i put in my email and uh no, if it's not uh i guess you would have to install it and so on. uh no no no, you see uh i thought you can put in here your email address and and just uh do this and then it mails me. yeah, you see i want too much. and after all matlab is also programming, so it mm-hmm. so. thank you for the information.
okay. yes. so you were talking about the two first question? okay. and a little bit about same people moving, with the clustering. uh-huh. so this one. what? i'm sorry, i didn't really get the point of comparing the pitch the pitch is the is the frequency that is okay. and the way you transform it uh v by the mouth? you were comparing what with what? uh-huh, okay, okay. so uh and you were talking about comparing what with what? okay. but you want to have the information because the the mouth, you move it. to scan t all the user. okay. okay, so you have the p the pitch and you have information about the voice dynamic or mm mm mm. emotion, the involvement in a discussion. if you are saying oh yeah yeah we should okay. in c_p_u_ you mean? not really familiar. hmm. okay, so you are the expert. careful, you are recorded. okay. okay, so all this stuff is done from the f_f_t_. also location to identity. uh mm-hmm. okay. mm-hmm. but to get a cluster the location that is uh a place where uh noise is regularly coming from. mm-hmm. mm mm mm. but for instance for a setting like uh here it would be enough, because we are not moving. okay, right. but we are uh targeting a place where we don't know i where it will be more flexible, so we need more than just uh location if we want to uh mm-mm. i agree. maybe just i think it may it may be okay, but it depends how long hmm. from th from the location. oh. mm-hmm. when are you planning to finish your okay. more extensible. mm. i don't know how mu maybe we would be more interested to know that it's not the same people, more than knowing that it's the same people. or to detect that someone is more that was not talking before is talking now. yes. mm. mm. okay. that are further directions. professor was interested to know when people were talking about him. ah, someone is talking about me, let's listen. and lighting a on his office. okay, so are you done w with what you wanted to presented us uh in regards with these questions? mm mm. three papers about? mm-hmm. okay. it's n no problem. no more time for problems. okay. uh uh. mm mm mm. slowly. might. for the first application we will do with uh prototypes, it will be more about quantity uh of speech or how long, how much someone speak, but when we will go to final uh details, if i'm talking and someone say yes yes yes from time to time, it show that he's participating, and obviously it makes different that if someone is talking and one is replying never uh nor moving or nor having eyes open okay. go through them. okay. no, i think uh i think we has already have plenty to read. try. and the application you are using, all of them uh y every time you are using matlab, or you are programming everything is based based on matlab? mm. mm mm mm. okay. i'm writing so only me can read. it's uh a kind of secret language. okay.
so b maybe i'll go back to your questions. um yeah. um yeah. and this also um allows you to separate people. um and the questions i did not talk about was uh going from location to uh identity of the person. um that is quite different, you you need to use a signal you have separated and uh do the type of analysis i mentioned. um from the spectrum you can transform the the magnitude spectrum and um um build model of the person. um yeah, pitch at least. but pitch is pitch isn't is not enough sometimes. i it's a frequency, yeah, yeah. it's a frequency of your vibration here. uh yeah. and then it's transformed through the mouth, that's the usual model. yeah. i yeah, um it's not necessarily done explicitly, but it's equivalent to that, yeah. so again, f_f_t_ is useful to do this type of analysis. um yeah, but it's quite personal. uh yeah. yeah. yeah, yeah. it it's an information equivalent to that. it's maybe n you don't need to do the whole uh complicated modelling, but no, no. yeah, rate of speech is also possibility. um how fast you speak, that's quite personal also. but it varies over time, um depending on emotion. that also might be interesting for you, detecting emotion. yeah, yeah. and us yeah, yeah, so these measures are a way to quantify that. uh p no. no, no it's not. what w w what is m what is more expensive is to um take the decision finally. so your decision can be for example uh who is it, or if the person moved, which is probably the most complicated thing, like the person goes away, then comes back ten minutes later, sits at a different place. uh you will need to build statistical models of um the person identity using these measures. so the ones um he mentioned um i don't know if you're familiar with that. yeah. yeah. so um no. yeah. s yeah. all i was saying is that these different measures are a way to uh evaluate the identity of the person. so if with location all you can do is extract segments of speech, for example, but it will not tell you that these are the same person. it's just a location. yeah. yeah. yeah, so that might be good for example for ten minutes, but then the person might move, or a different person might come. yeah. it is. well, unless sometimes somebody goes there or yeah. yeah. mm-hmm. yeah. yeah. yeah. so it hmm. but that you can do with location, if it's only on the short term. on on the off-line yeah. i would say you might use these measures in a simple way if you do it uh on-line. like um to give feedback. but uh you you mentioned that another side of your project is also to analyse off-line. so there you might want to use a we had a student here, he finished uh one or two years ago, but he left his software, and his software is precisely to do that to um cluster at uh another level, to cluster these small clusters of speech, uh group them by person automatically. y no, from the these measures, pitch, etcetera. so um yeah, exactly. so i would say from location at the lower level you can get small parts of speech, and then at the next level, possibly off-line, you can uh group them into um a speech segment. i'm hoping to have enough time to try um doing that before i finish my thesis. um so if i manage to do some um uh practically usable software, i'll tell you. uh in five, six months, abou approximately. more busy i would say. yeah. so you said you want to do separation, um but ex exactly, yeah. yeah. okay. okay. ah, you want to do voice recognition. you might want. okay. okay. yeah. so yeah, once you have locations it's not a big problem. um mm-hmm. no no, you should separate them first, yeah, yeah. so so the type of things i presented can lead to separation. um it's only a very small step to add. yeah, yeah. although it might be, if you want to um ex extract these features like pitch and uh rate of speech, or energy simply, from each person. um you might have to do some ba basic separation. 'cause quite often people uh don't realise that they talk at the same time, um interrupt each other. so even if you don't want to recognise a speech fully, at least um you need yeah. it's okay. you can email later. yeah. um so yeah. uh okay. a semantic context, yeah. yeah. yeah, then mm. hmm. so it's uh yeah, you could detect one word, yeah. keyword spotting, yeah. so um yeah. yeah, i i'll just say uh if you want i can point to um three papers, um but no, i think it's better if i just send you the link, it's probably simpler. uh one im is about this um sector based stuff, like uh the localisation and detection. one is about how to take the decision, that somebody's active or not. um it sounds simple at first, you think you just put the threshold and that's it, but um the problem is uh the it no, you can do that. there's no problem, but i if you want your your system to work in different conditions, like cafeteria, library, um the environment will be quite different, so a single fixed threshold value might be a problem. s yeah. so you can do that automatically, this kind of things. um i don't know if you're doing that already, but uh okay. so uh i have another paper which might be interesting for that for a single channel uh calibration. and i've code on-line, so for this particular one. and the last one i would say is um the clustering um to um yeah, the clustering of the different locations over time, so that you can get small clusters of speech. um there are other ways to do it, but uh i think it's it's important because um in spontaneous speech all these words are quite small. so you need to do that uh adequately, yeah. but otherwise yeah, i'm done. hmm. yeah, yeah. many small events basically, yeah. yeah, basically uh i mean it depends uh yeah, even if somebody says just yes. i don't know if it's important for you or not. it might be. yeah. yeah. hmm. yeah. y yeah. yes. um so i i'll send you links some of the papers are long, but you can just um briefly look, yeah. um we can go and see olivier, unless you want to talk about something else. huh? well, we can try. but uh we have yeah, we have o_c_r_, but maybe not on this computer. yeah. that is good idea, yeah. it's a good idea. yeah, yeah. yeah. once i i um coded the most expensive part in c_ just to see so you can have a mix of both. from matlab you call c_ and back. but the day you want to do uh on-line real time stuff nah, i wouldn't wouldn't trust it.
