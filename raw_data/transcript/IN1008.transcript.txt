okay. okay, uh for for uh the noise sensitive table project um we decided to um to take the noise uh as input, and uh give a feedback uh with leds on the table. so this is uh one of the the possibilities that were offered to us. but uh we decided to do to do uh it this way. yes, the the first uh the first idea of the project uh was to have um the whole table covered by leds. but uh for the prototype of this isn't this is not the the way we implement it now. but uh in fact we have a there was th yes, th there was the the restriction that if we cover the le the table with leds, uh some won't be visible because uh you have uh yes. so um if there is information displayed under the the sheets uh the it won't be um very useful. so we decided to have it uh yes, thirty centimetre uh at each side. weeks.. this is for prac practical reason and uh also a matter of time. because uh yes. but uh th there are o other problems, like um yes, the the information display on the sheets. so we don't know if it's yes. true. and there is uh a lot of programmes now about this implementation. is it's not uh it's not obvious. hmm? still in progress. almost done. mm-hmm. yes. mm yes. yes. uh i have one or or two question. i don't know if you want to to begin with the the explanation about the the release. o okay. so um i have actually two questions that are uh just practical question abo about my um my implementation now. uh i no. so and the first information is um about the the noise level. n not the noise level, the sound level. uh because uh what you get from from the microphones is uh a curve. and um a way the way uh it is implemented now uh to detect yes, you have a uh i have packets uh for uh let's say uh this is uh this is uh not uh fixed. but uh i have packets for about um uh alpha second. so it's uh second. and uh on these packets i want to know the the sound level, the average sound level. so uh the way i do it now is uh to to take uh each top of the curves' bottoms and uh just making leverage this. but i don't know if it's it's not really um the best way to do it. but i don't know uh if yes. okay. so this is this the the first question. yes. but i think uh this is two different problems. uh the filtering must be must be done before. and after that uh if you have uh a cleaned curve and you can uh just apply uh functionally. and uh the second question was about um uh filtering and uh f_f_t_ yeah, what you can get from it. uh general questions. okay. mm-hmm. uh o yes, i will just add something. um is there a way to uh to from one uh from one stream to uh to build two streams that correspond to the the voice of one people and the voice of the second? the yes. no. and it it this is not uh a sound question. sound related question. line in. uh yes. the the communication between the the extension and is uh of the. no, no, no. uh okay. and the fire-wire port. and there is an extension for this this uh this device connected to it uh with uh optic fibre. yes. uh yes. be because for now uh i'm just sampling the sound that uh eight thousand. so uh just uh something very very rel i could uh even uh go to lower. but uh this is just to get the the s the noise, the sound level. but uh if i want to record the conversation with uh uh a processing after th yes, to to have a better sound quality. uh perhaps it's better to have a x_ array, yeah. something like that. yes.. okay. mm-hmm. sure, sure. but uh what is the the gain? in terms of uh position? uh uh uh centimetres or uh more? okay. m uh what you get is uh the real position or uh just uh the angle at which the the sound comes? mm-hmm. mm-hmm. okay. yes, sure. yes. uh because uh there there is just um four microphones that are eight. okay, yes. okay, but there are in uh in a plan. okay. because uh at at t_p_f_n_ there is a lab that is uh has done almost the same thing. but they have uh eight microphone that are uh at uh yes, the eight um the eight uh uh cor corner of uh a cube, yes. the elevation, no. but uh what is could be relevant, it's not uh almost it's not always the case with the the um yes, the the position, the the radius of the yes, it could be. yes, okay. yes. because um because i the um the distance between the the microphone is too uh is too low to mm-hmm. yes, sure. yes, but i think the the most relevant part is the um the other uh the hardware that synchronize the microphones and that. yes. yes. mm not maximum. the the average on the on the packets. uh yes, the the i take the the b i g when i go up to the the curve uh i'm uh on the at the local maximum. i take it. then the next uh local minimum and uh taking this uh this range, doing average on it. yes. difference. difference is uh is that if i just uh mm yes. yes. okay. a you have uh you have fifteen implementations. uh yes. okay. sixteen. sure. okay, okay. yes, and then i just uh because uh we are using um uh an open s an open source framework to do this, there is uh something, i don't know if you know i know it, uh calls sphinx, that is doing uh s sorry. yes. doing um voice recognition. and uh i just use it for the for the for the microphone and the acquisition. so uh there is uh already uh an f_f_t_ uh module implemented in it. i didn't test it. but um yes, sure. y yes, for the for the the position,. also for detection. mm-hmm. mm-hmm. 'kay, yes. sure. okay. yes. no. yes. okay. y you have to detect uh an event on one microphone and uh know when it's happened on uh on the other one. no? okay. mm-hmm. mm-hmm. it's related to the number of microphones around. no. okay. mm-hmm. okay, this is a measure of the activity o okay. th there there is a distance. but it's not very uh very effective. as as you mentioned. it's and uh from from this kind of if information you can uh let's say have uh a finger print of the user to know uh if this is the same user speaking. so not sure. because the the um okay. which range.. yes, okay. so this is this is a bit arbitrary. because uh perhaps uh person two will have uh some frequencies in the range of okay. and if you reconstruct the stream from this uh this range, you will uh get something uh that is uh okay. mm-hmm. and you can you can um this way you can filter uh perhaps some range that are uh just very low frequencies for uh perhaps uh a noise or the yes, the fan. okay. yes, but but then this this is very interesting because uh this way you can detect uh if someone is talking, if someone uh has an uh some noise with uh perhaps putting his uh his cap on the on the table. perhaps if there is a laptop and uh in one position and you can classif uh classify. and this is yes. she' very good. okay. yes. yes. and the the length of the time frame has um an inference of uh on uh which parameters okay. th there i uh let's say if you use a one second window. uh the in fact the the frame lengths will be uh very uh very uh i yes. okay. they they won't be visible on the on the wavefo okay. yes. but the the the the long uh the longest frame you would use would be one hundred. at most, okay. okay, yes.. 'kay, so you can yeah. okay. enter uh when user is uh yes. and before i forg forget it uh i have one question. uh you say you do the processing after uh afterwards. and uh what is the the cost, yes, the the length? one one or uh bit longer? okay. yes, but matlab is very uh very effective for uh matrices and uh okay. yes. okay. yes. relative, yes. mm. yes. and even if you use uh four microphones to get uh not good. okay. okay. and eight is uh is good. there i yes. but uh yeah. yes, one, two degree is very very small for uh for our application. because we want we really want to detect uh if th this is the same person that is speaking or not. if there is uh an error of uh let's say uh ten, fifteen degrees and but uh it is um capable of uh seeing it as two different person. there is not problems with the okay, and the number of sectors is uh is not dependent of uh the number of microphones? okay. if there is uh two two person the same sector, i see. mm-hmm. mm-hmm. no. geometry is not. we have not uh thought a lot about it. yes. f_f_t_. but th uh uh i think what is very important here is the synchronization of the microphones. right. okay, um uh i'm speaking about it because uh we wanted uh to do this table in uh a mid-tech way, let's say a cheap way. to have not uh very expensive tables. so i don't know if um uh what i've seen w on our website was uh that the this installation was uh a bit expensive. uh but i'm trying to uh to to to see what is very important and uh what is not uh for yes. sure. yes, sh i think you have to uh to uh it sh these are these are um some kinds of uh and this one. yes. okay. expensive. because because uh of the quality of the the and they are plug in the on the x_l_r_? or uh yes,. okay, for the our we'll we we'll uh. mm-hmm. let's have a break.
no. i don't uh well i guess it's just a restriction we took. so so we don't uh the m amount of money we have to spend. otherwise we could have uh we could imagine to have a whole table of of uh s well, of l_e_d_s. and then y you can really start to to some regions of the table. so yeah. oh let's. does yeah, i can just explain my project. it's completely different from this one. well, it's also about tables, but the idea is if uh uh the several students come to a table with their laptops, they should be able to to share information easily. and uh in fact w what we we are doing now, we project um another screen on on the table, and different people are are are able to use their their mouse of their laptop to go to the to the project screen there and uh the project screen is just another windows machine that is running. so you just have another f computer more or less you can use, but you can use it with your your own mouse and keyboard. exactly. but uh in contrary to to having al another keyboard and mouse that only one person can use, now the different persons can use the the screen um at the same time with the with the mouse. i'll s still the same subjects. it's it's collaborative work. but uh it has nothing to do with uh audio processing and uh l_e_d_. so anyway, uh i think the the details of the implementation is not that important. it's more important that first we want to to have some events like two people are talking or one one started and another started and uh well, more or less broke into the speech of the first person, and so on. and then you can start to visualise these uh different situations. well that's uh i guess it's first prototype i you see we start with uh people two people talk or and don't and don't they. uh afterwards uh one could imagine that you for example you can detect that someone is a bit angry or is uh yelling. and wi with different colours with uh with different colours on on table you could uh display this this uh give some feedback. but uh yep. that's uh once we get the hardware. mm. yes. everything from um multi switch administration of uh audio and how you can uh analyse it, uh what information you can get out of speech and so on. so probably it's uh it's f be a good idea if you have s two uh with um with source like uh books or web page centre. give a lot of information how it is audio programming and so on works, cetera. maybe you you know some at least a good book that's uh dealing with this stuff. mm-hmm. and is possible to do this in real time. that's uh no. yeah, but what happens if o someone is standing then? yeah. no, no, no. in fact uh that's uh it's another thing, this optic. well uh m we have ye actually, it's not really an extension to the first box, just an indu industry standard uh uh protocol between these two uh well well there's a special protocol to to deliver uh audio signals and uh its path through uh o um well actually, just to have the the icsla e uh possibly to to branch icsla. hmm. no. uh-huh. well uh so you say this one already works to uh to tell the the direction. um would it also work if you placed uh the microphones at at the border of the table? you see, now you you have it quite central. and uh what happens if if you put them here on around the table? would it work as well? or you just didn't try? or mm-hmm. yeah. yeah, it is clear, this. no. mm-hmm. no. you lose information. um. yeah. well that's something with f_f_t_s. and the phase. so the magnitude you don't really use? or and for this uh well uh uh uh i did a speech processing uh course and i'm not sure if they a uh they also um mm learns that that. we more used the face than the magnitude. so uh i guess the uh-huh. mm-hmm. and what happen uh if two person are talking at the same time? this it just get two sectors that are higher than others, no? sounds. well uh uh mm-hmm. yeah. no, it's more than. no, it's uh mm-hmm. i'll show you this afterwards. now if you don't really detect the pitch, ch can say that uh no persons.. it's not really meaningful. well if you do this. somehow. mm-hmm. mm thanks. well, otherwise it could also just rely on the frequency band separation and detect that way the different persons and th then you don't really know where around the table these. but uh well d depends with how you do it. but if if you place the the microphones uh n yeah. uh that way you can separate where where is the the voice loud with uh or the loudest. well it's quite different thing and uh you don't really get uh a name for it. but uh at the end it it depends a bit what you want. if you are i guess it's completely other approach. so uh expensive? ca can you uh say a number? okay..
okay. should i start? maybe by presenting the project little bit. okay, so historically it was um uh last year prof delambeaux in uh his c_s_c_w_ classes uh asked the student to design a table. and all the students, and guillaume wa was a student of this class, was very excited uh to design such a table. so he decided to um to make more research on this topic. so the general topic uh is about uh interactive table, um disappearing computer, um what is the other one. um um na uh in the general fi field of uh collaborative computer supported collaborative learning. so the idea is we have different people collaborating uh in a pedagogical purpose. and uh we know that when we put a computer in front of them, uh it doesn't help focusing on collaboration. so if we make the computing um functionalities disappearing on on the in the furniture, in the wall or other things like that, we may more efficiently support uh collaboration than the the learning. so i arrived uh in june, something like that, a couple of month ago. uh guillaume and uh michael uh are two students that are working uh four four months to their master project. and they start working on two uh different uh sub-projects. so guillaume is working on the noise sensitive table. it's uh what we are discussing about uh today. so the idea is uh okay, w we have in the lab a notion about um root mirroring. the idea is if we can provide a feedback to people working together, uh it may help them self-regulating their activity, their collaboration. so could be normative or not. in our own case we want to have uh a table that would be accessible by by students. uh at o_p_f_l_ we are uh at o_p_f_l_ uh the learning centre will be built in a couple of uh years. so the idea we we will have plenty of rooms uh like in libraries or meeting rooms where student will arrive and work. so if we have this table uh that is accesi accessible to students if they arrive, if we provide a feedback of um who is talking, um instantly the uh at one moment or uh through time or the turn taking, uh maybe they will just by seeing an explicit uh feedback of that, they will um take profit uh of it and um yes. the idea is to have uh peripher peripherical uh information. we don't want people to focus on uh what's going on, uh what on the feedback. so we don't want histogram or very precise information. but we will uh have leds that will be distant. there will be six centimetres between each led with a blurring glass. the idea is to have more um a kind of uh art, artistic uh feedback, to have a kind of general feeling of what's going on. something about uh dance. for practical reasons. we kept uh one paper sheet. so uh the first shape of the table the shape of the table obviously is one of the factor on that will change the collaboration. so uh but we don't want to start with too many parameters. so we start with a table that is approximatively the tab uh of the table where we are now. and we will have uh a square shape like this that will be here and a second one that would be there. so we will have something like this. like this. that will be the our first uh real prototype. okay. and the so this l_e_d_ modules are being uh designed uh now. we should have them in a couple of weeks. and uh we will be able to uh start uh evaluating uh the prototypes. uh uh about the c_s_c_w_ class, that will start at the end at the beginning of november. uh so the student will be asked to design uh a table. so to find the shape uh of the table to get uh who the and put that uh on the um on table uh legs. and we will uh so at the beginning we wanted to put this l_e_d_ modules uh in the tables. but we won't make it because uh for solidity reason of the table. you know, it's not very uh strong uh wood. um so we will rather beam uh the l_e_d_ on the table. so we come back to a more richer display. b but we will um restrict it to a small uh to have the same display. led lights uh that will be distant uh six centimetres between leds. so we will simulate our and we have yeah. and we have the feeling that to have the information embedded in the table, make it part of the table, it i don't know, we'll we we will try uh we don't really know what uh will happen with this prototype. we are very excited about that. okay, uh maybe before uh going on on this subject we can talk talk a little bit on uh huh. so based on the fact that most of the p_f_l_ student have their own laptop uh the idea is the similar idea uh is to okay, people are coming on a should be able to very easily uh use some um resources offered to them. so so the idea is to help them socialising, organising their own work, and uh not in a classroom setting, but in a social place where they could gather. different directions. okay. so uh maybe we can focus a little uh uh bit more on the what we have done about uh the noise sensitive table. tell you about uh what we're interested in on the so we start so guillaume started by uh developing the application that get the audio input. so we want it to have a modular architecture to be able to improve each uh layer separately. so the audio input uh is quite basic, no? we u we use uh sound cards to to get uh the signals and to detect to uh on to ha and we have a threshold to detect uh when someone is talking, at least when the noise is uh at uh is bigger that uh specific levels. and then we get this information about people talking, not talking, and to say okay uh, this one is talking for x_ time or uh has been talking uh x_ percent of the five last minutes. or two people are talking together. so this is more semantic uh layer. uh we call that interaction uh models. and uh uh so when different conditions are trigger are detected, we trigger on the interaction event. so very simple one could be one person is talking. so it's uh the easy one. it's still in progress. still in progress. so the last layer is about this visualisation. w we will uh we are thinking about the visual grammar, meaning that when we have an interaction event that is uh fired, we uh will associate uh a way of visuali visualising it. so for instance if someone is talking, uh we can just have a light that will centre on the point that is uh close to him. that could be an uh instant uh feedback. and if he has been talking maybe seventy percent of the last five minutes, as me, uh uh it it may be reflected as the um intensity or size of a s a cycle. centre on the same point. and then if there's turn taking, we can show waves of light going from uh me to you uh or from other people. or using the centre part of the table to show some something more about the um uh more general uh dynamic. or laughing. yes. in uh we can change inte so we have uh eight by eight uh two times leds. each uh led is actually three leds um. th uh m_g_b_ led. and we can change the intensity. so it's already quite a rich uh feedback. yes. and just a last thing. so this table is uh two different things. it's uh one object that will uh g provide a feedback to the terminal users, people collaborating around uh the table. but it also a tool for us as a researcher to test uh both the interaction event we want to detect and to test the visualisation. so um we will be able to edit some uh interaction rules and the visualisation uh grammar. and uh we will be able to process to post-process some conversation just to see what happens and to uh have these um models on feedback working when people are using the table. so i think it was yes. um maybe we could summarise the question we were thinking about. so basically what we want we want to know when a_ is talking, b_ is talking and uh basic information we need.. maybe we can uh go through the different questions and then you can organise your own? um i would just add something. uh we are not interested so much in sound level but uh in the level of uh the sound that corresponds to voice. mm-mm. okay, it was just a okay. go on. and i would add uh one more. it's about um because what we can uh record is uh sound uh origins. so in po uh so no sonic points. but what we are interested in is people. so the question is if someone is moving, i'm talking here, i stop talking, i'm going here, i'm talking. how to detect that it's the same people. if we want to uh integrate information to say these people has uh spoken seventy percent of the time in the five last minutes. obviously we need to uh to know that it's the same people. so right now our practical solution is to have one uh microphone close to um every place where people are supposed to sit. so we can infer that if a sound is coming in this area, it's the same. so kind of trick. but for the beginning it's enough. and a last question that is related t uh our last if i not find another one, um about people, if someone is leaving or if someone uh maybe it would be something ev uh quite complex even for you. if someone is staying here, but is quiet or leaving, how can we detect that someone is here but quiet? okay. i know. but how to integrate the information? uh not not yet. maybe later on. but oh, the wait and the sit.. or putting his bag on the okay. but it was just to it's a question. i'm not sure we will talk too much about that today. uh not directly. but it's uh related. so in a couple of days we will get this um uh the fire-wire uh box. so we will we have chosen one that has eight uh line up. so we'll be able to have line in, sorry. so we will be able to plug uh up to eight uh microphones with uh jack plugs to exhale air with the possibility of extending with eight more x_l_r_ uh microphone with another box. so we ha we have the computer, we have the the main box with the eight line ins. and it's fire-wire. it is optic. to have eight more x_l_r_ if we need it. but we are not sure to to buy it now. but we can extend it. that's uh yes. well we are not sure about what kind of microphone we should use. so we wanted to uh that's a first thing. so we wanted to have open possibilities. and secondly, maybe we will use this box, you know, to get uh data. i don't know if in another experiment, maybe in another project, if we want to have comments of different people around the table or o elsewhere. uh getting uh having the possibility of using x_l_r_ microphone could be uh uh could be neat. so everything is open. no, we are not very interested in. more than one mm-mm mm-mm. okay. they won't have nice light uh lighting on in front of them. they would maybe careful. but okay. so maybe w uh you could present a different work uh you have done. and uh i don't know how they are. so you told us it was different pieces, more than one integrated device. oh uh what would be the d difference between detecting the average or detecting the maximum? what what would be the difference? what is the difference? it's a kind of filtering already? sixty? okay. again. uh-huh. okay. so it's thirty two millisecond frames taken each an idea. so something like that. so why are you overlapping? i have a rough idea. but uh what is the main interest? mm-hmm. yeah, and it's kind of crappy. precise. okay. okay. mm mm mm. so you use the first and last point to detect what is between. mm-hmm. mm-mm. okay. so you are mm-hmm. mm-hmm. corresponding to breath or a little noise. 'kay. but you're talking about the microphone array here? strong background noise. so phase uh-huh. uh-huh. okay. so the phase the time between uh audio signals for um for this microphone on this microph okay. okay. mm-hmm. mm-hmm. and do you get something about the um distance of the guy that is talking, or just a direction? okay. mm-mm. mm-hmm. okay. you mean that each person will have a specific frequency because i the way he's talking or his localization? it's a kind of uh speaking styles? both. i don't really get your last uh diagram, yes. so it's at one given moment still? okay, so you can say that f this frequency belongs more to this person and this one more to this one? but some frequency may maybe are not used neither by uh guy the guy one or the guy two. okay. maybe they are using the same frequency. so you need to s okay. okay. okay. and can you detect if someone is laughing or angry d uh there is kind of signature uh even if we can't really trust it uh uh s one hundred percent of the time. uh and do processing on it. okay. pitch. it is pitch. li okay. what is a? excuse me? uh-huh. okay. okay. so where are you represent the phase?. okay. uh-huh. uh-huh. okay. mm-hmm. mm-hmm. mm mm mm. because if we are distributing the microphone all around the table, then we come back to the energy solution? is it what you say? with the energy dissolution or with uh f_f_t_ too? okay. but there are many many things. the difference between the mm-hmm. mm mm yeah. what would be the difference? you will not compare each mic with uh all the other one. but just comparing this one with the mm okay. okay. but what are uh-uh. so what are this mic for instance, the electret mic. omni-directional? electret? okay. yes. and when you were talking about comparing energy in the case where a microphone would be distributed all around the table, uh you're comparing energy and the time delay? because time delay is is too short to be compared or can be neglected? okay. okay, okay. yeah. it's time. cool.
okay. mm-hmm. okay. okay. mm-hmm. okay. okay. yeah. okay. yeah. okay. mm-hmm. okay. mm-hmm. okay.. ok okay. mm-hmm. mm. so do you know uh which way you give the feedback? light or i saw something like that. okay. hmm. hmm. mm-hmm. mm. okay. okay. yeah. maybe more complex, yeah. yeah. so th hmm. but yeah. to the border basically some distance? or okay. o okay. obviously, yeah. okay. okay. hmm. yeah. yeah. okay. right. hmm. mm. okay. mm-hmm. okay. but don't yeah. don't you think with a beam also you have less problems with the sheets? or you don't want to do that. yeah. okay. confusing, yeah. hmm. okay. no, it's a choice. yeah.. okay. mm-hmm. yeah. mm-hmm. okay. yep. okay. yeah. but physically it's just uh something flat on the table again, yeah. okay. okay. yeah. okay. hmm. okay. yeah, so it's different, but still uh yeah, it's same okay. okay. yeah. okay. yeah. mm-hmm. okay. mm-hmm. okay. mm-hmm. mm-hmm. yeah, yeah. hmm. right. hmm. okay. right. mm-hmm. mm-hmm. or too long. oh, just kidding. okay. yeah. yeah. right. mm-hmm. yeah. ah, so you ha you you might have multiple colours? or yeah. sorry, didn't get that. yeah. yeah. alright. okay. yeah.. mm-hmm. right. mm-hmm. okay. mm-hmm. okay. hmm. no, it's quite clear. so uh i guess you want to know what you could do with an array maybe? mm-hmm. yeah. or yeah. yeah. yeah, it's good idea. just give some questions uh mm-hmm. right. mm-hmm. mm. mm-hmm. okay. mm-hmm. yeah. mm-hmm. you mean the top of the wave-form? okay. yeah. oh okay. hmm. hmm. mm-hmm. okay. yeah. yeah. hmm. okay. mm-hmm. yeah. yeah, yeah. okay. separation uh yeah. hmm. mm-hmm. well you need a camera. obviously. i mean will you have a camera? maybe you can detect breathing. yeah. yeah, that you could do. or on the table uh there should be well. okay uh okay. mm. well okay. mm-hmm. okay. so you i think guillaume said that's optic, something like that? or okay. so you you need a different you need a microphone with um a device inside, right? ho how does it work? okay. yeah. okay. mm. hmm. it's another fire-wire or not fire-wire, but um okay. but concretely you would use that to get higher quality signals? y that's all.. yeah. ah okay, okay. yeah. yeah. hmm. okay. and how hmm. hmm. you mean you meant eight kilo uh sampling frequency? yeah. uh we use sixteen. so that's eight kilo band-width. sixteen kilo uh sampling frequency. it it's not bad. uh it's quite decent. um mm f before answering the questions uh th there's also a point of uh precision of localization if you use an array. so if you use higher frequencies you can get more precise localization. but maybe you don't need it. uh that i can't answer directly. uh it depends on your set-up, you know. um you have to test basically. um y you get uh the angle. the most precise one is the asymmet in the horizontal plane. then you also get elevation. but it's not very precise. it's more of an indication. and radius is very bad. if you use a planar geometry like this one. yeah. um you can use multiple ones and do some um um triangulation. eight, eight. these are only screws. microphones are on bottom. yeah, yeah. mm-hmm. a cube? okay. yeah, then you might get uh better direction in elevation. but i'm not sure it's really relevant. um no. so uh if people move forward, yeah. ah, the radius? th then the r the problem is not the geometry, it's the um i you need more than one basically. with one you will only get direction. microphone array. so mm. direction, yeah. um yeah. it's a different option, yeah. um we haven't tried that because we are not going to have the special kind of table. more uh this is not a good example but we have like a box, just bring the box and put it. so it uh we could not consider that. but in your case it could be interesting, yeah. unless people put something on the microphone. but um yeah. right. but that i'm i think after the meeting we can go and talk to olivier about that. i'm not the person for hardware. yeah. yeah, i'll t i'll try to present uh i guess it corresponds to your questions. yeah. so the the lowest layer would be the first two questions. um you asked about how to detect um activity. currently you're thresholding the t maximum of your wave-form, right. ah, ah. okay. and you're looking for a peak. yeah. yeah. yeah. hmm. okay. uh yeah. yeah. and your second question uh was about f_f_t_, what can be done with f_f_t_. so um the approach we're taking here is to answer both questions the same time. we use f_f_t_ to do detection. we don't use the wave-form, the raw wave-form. so um we split um we split the signal in small frames, like sixteen millisecond. sixteen. um each frame is taking thirty two millisecond of signal, just to give an idea. and there is a one frame every sixteen millisecond, and they overlap of fifty percent. um yeah. each sixteen millisecond. well th these are details. it's just to give an idea. um well it's uh the the c because usually when you do f_f_t_, y if i'm getting into details, but you you apply um a window to avoid effect. so the beginning and the end it it i it's not crappy, it's just not very much rep represented because you apply um a window which has this shape, having window. so they're very yeah. no, i'm saying uh when you take your signal you take one frame, you apply the window, uh and the window is simply uh coefficients you give. and you give higher coefficient to the middle than to the extreme. so if you don't do overlap y you will lose information at the beginning and the end. okay. but you don't have to do overlap. i mean uh it's um hmm. um c_m_u_, no? is it from c_m_u_? um uh yeah. yeah, yeah. right. yeah, yeah. yeah. f_f_t_ is only a tool. uh um uh what we do release a phase domain analysis. we only use the phase between the microphones. um yeah, but also for detection. yeah. um we have a way so um i don't think i should get into details. but basically the beginning is your signal which you slice into frame. you do f_f_t_ on each mic. and the end is um a number of uh frequency beams which are used by each person, to uh explain roughly. s so when you speak, speech is wide band. so um the more active you are the more band-width you use. so you will get a large value of band-width for people who speak. and uh for the others it will random. it will be a small value. or just background noise. and so uh yeah. 'cause one problem if you use energy based methods, which is probably more uh the thing you've been using so far, is that it's not um quite related to location. so you can have um uh for one person you can have uh a high energy signal which is difficult to locate, and visa versa. um i think in your case you're quite interested in the location. so i would advise to um use more uh phase domain methods. um once you have the fifty, for each frequency you have uh the magnitude and the phase. so you can compare the phase of the microphones. and this is directly linked to the direction of the person. um sometimes we use it, but not it's not the first thing we use. i know it's a bit counterintuitive. but um maybe i'll go t to the board. yeah. okay. r right, for a single channel it's not very meaningful. but here it's relative phase between the microphones. so uh i'll just try to um s summarise. no, that's what i don't do. um this would be valid for uh like speech recognition with a single channel. that would be a very good idea. but if you choose to use multiple channels um let's say you have four of them, with f_f_t_ you can just uh, as i said, l look at the phase between the microphones at a given frequency. um it is linked to this value, yeah. uh basically the time you're mentioning is this value. and this is the frequency. and this is an angle in radians. so uh f_f_t_ gives you a measure of these values. uh this, i can point to a paper, uh i don't think this is the right place to explain everything. but will give you if you look at your table, uh what we have developed here is an approach where um th uh you divide the space in sectors. for example ten sectors around the table. and uh in each sector you will get the value. no. it's uh application dependent. for example um well this is not. and on this one you could have a large value. on on on those ones, small values. uh these are number frequencies which we estimate with several steps from these measures. so to conclude what we are doing is uh we estimate how much of the frequency spectrum you are occupying when you speak, or i am occupying in this direction when i speak. and it turns out that um this is quite good to do uh detection and localization at the same time. bec because you know in a sector space there is uh uh this much activity. yeah, yeah. um more recently um i've worked on uh um prolongating this with a more precise direction evaluation to know where in the sector the person is. so that uh that's not much work. once you have done this, this is can be this can be done quite quickly. then you yeah, y well i i use a value. but then you'll get two large values. yeah. so no. just direction. yeah. well, f yeah, i it's not uh i it's an inherent problem to the geometry you use. you can for example, if you use another array, uh you can intersect lines of direction. uh mm maybe. uh uh i w mm i wouldn't trust it too much. classically, what you do is you extract the signal of the person. um one interesting thing is that these numbers are not uh arbitrary things. they represent the number frequencies where a given person is dominant. but when you have two of them, which might be interesting for separation, you know already when you have done this processing uh w which part of the frequencies of the spectrum um belongs to this person and to that person. and then it's easy to separate the signals. um both. it's it's more an instantaneous um this is still instantaneous. this is for one time frame. if you look at all your frequencies for example uh zero to four kilohertz, um you can split it and say uh all these parts belong to person one here. and uh all the other parts belong to the other person. that's correct. but then uh you can look statistically it wi it will be negligible um because the r of uh of the difference level. yeah. it's uh you can listen to it, yeah. and then you can do some uh maybe higher level um analysis where you get the pitch of the person or this yeah, yeah. and this is exactly. yeah. then it's uh random. and that's why you get these values uh which are random. yeah. yeah, but this is very rare. and when it happens, one is always masking the other in practice. so um yeah. no no. but uh i've i've seen quite a few papers i i've n not done it myself. i'm really on the lowest level. um but uh once you have done this, as i said, you can uh separate the signals and um do do processing. uh so uh you can get pitch, rate of speech. that's quite easy. pitch um the uh uh timbre en francais. um so uh for different person this might be quite different. at least for male females. and uh rate of speech. like if somebody is talking very uh in a very energetic manner, it might be quite fast. uh or just energy also. one yeah, also. um that might be an issue with people are bringing laptops. um they will b yeah. they might be detected as another another source. uh so you would have to uh classify the sources as human or machine. but again, yeah, i think once you see the spectrum, if it's just pure and stable uh also yeah, if if there's no pitch. so uh yeah. yeah. so all will go and be detected there. yeah. so w what you're mentioning is a pain for us. but for you it might be 'cause we're only interesting in getting the speech. but for you it might be quite uh quite good, yeah. so i've to avoid uh filtering and smoothing. 'cause as soon as you do that um you exclude some of the information. so it's better to keep it for the latest stage. so um um on top of this there's another part uh which is more linked to the tracking. all this was for one time frame. so um yeah, you can play with it. um in speech it's uh stationary over ten, twenty millisecond. and we make a stationary assumption, a local stationary t_ assumption um which allows you to use f_f_t_ and blah, blah, blah. uh now in spite of that i know some people use much longer windows, which is not a problem. but uh it's a bit too much i think. uh like some very small words, like yes, they might be two hundred millisecond or three hundred millisecond. um yeah, they might be blurred with uh silence. so yes. you you might have to play with it just to save processing time, like use slightly longer frames. yeah, at m at most, yeah. um simply because i've used one hundred as a minimum length of a speech word. yeah, so. um now assuming you have done this for each time, i if this is your detail or i should use a different um if this is your asymmet at each time you get a direction. so asymmet is your direction in horizontal plane. it's an angle. like north, south, east, west. and yeah. no no, of this is uh really different. i'm just saying that uh once you have done this for a given time frame you can have a direction of the person, at least in terms of sector. and it's also possible to give a more precise direction quite quickly. so uh this was kind of the lowest layer. now this is a layer just above it which might interest you. um if you repeat that over time you will see patterns um for example uh at two different locations. two different person will speak. so you can cluster those. and you will see that uh if it's long enough it's some significant event. um this can be done quite cheaply, yeah. yeah. yeah. a cost. yeah, th um well i use matlab. so it's not perfect. it's like three, four yeah. but not everything is uh um simple linear. especially this part. it's definitely not linear because you're looking at a maximum energy in a frequency, kind of. the most expensive part is here. with matlab i have three, four times real time. um you can do sub-optimal processing and uh for example here i'm considering all possible pairs of microphones, uh which is twenty eight, and the processing is directly um r proportional to that. so um you can save on that. but then you lose a little bit on precision. also i i'm using all the small frames with fifty percent overlap. i don't think you really need absolutely to do that. it's not that good in terms of uh direction. um yeah, i would be careful with that. like five, six is uh decent. uh six i would say, yeah. eight, yeah, uh i get down now to one, two degrees uh root mean square error. uh so you might not need that. that's what i mean. might not be um yeah. yeah. yeah. yeah. yeah. so it could be sufficient to uh, yeah, define enough sectors and no. no. so it's arbitrary. i use twenty degree sectors. uh 'cause i had to choose a value. but it's up to you. um yeah. yeah. um so that's another way, yeah. it it's almost like having a lapel, yeah. um yeah. i think it's not necessarily a bad idea, actually. um well uh if you are able to calibrate your microphones um the ones who are closer to the person will get more energy. um so you can compare that. it's possible. so yeah. um i have experience with that. um on the side i've done some single channel work which can maybe help. um 'cause uh it seems to me that you are not definite on the geometry, right? yeah. um m this part is quite specific to a microphone array where like they are concentrated in uh some place, like in the middle of the table. yeah. yeah. yeah. but you can kind of come back to that also. um uh f_f_t_ also. yeah, yeah. r right. yeah, yeah. it has to be as good as possible. yeah. yeah. true. yeah. we can also consider directional microphones. you know no. um so you can uh their pattern is uh not uh the same uh depending on the direction where the person is. so for example if there was a directional microphone pointing there you would get most of my energy but less from you, et cetera. you can also do microphone arrays with directional microphones. uh um might not be relevant. but um uh more or less. it's it depends on what you want. i mean uh i don't know. yeah. they're o yeah, omni-directional, yeah. m uh i guess so. it's the same, yeah. exactly the same, yeah. yeah. yeah, yeah. quite expensive. yeah, they are um high quality mics. uh i think we can leave this question for olivier. yeah. no, no. y you would uh assume uh you would neglect it. the time delay is is very small. it it's only used for uh getting the a direction when you have a microphone array. but in that case it's not relevant. um you would assume that they are roughly synchronised, not necessarily very precisely, and um compare the energy levels. but i have no experience with that. um now i don't know, you don't want to use ah, it's almost okay. yes.
